{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook walks through an engagement with Wellcome Collection's catalogue API for the purpose of working with the digitised text that Wellcome makes available.\n",
    "\n",
    "It is an end to end example, starting with accessing, cleaning and working with catalogue metadata, going through the process of acquiring the raw text of books held by Wellcome Collection and ending with the very basics of topic modelling it. In a way, it walks through an initial digital engagement to get a rough feel for the possibilities of a dataset and a digital method.\n",
    "\n",
    "The intent of the notebook is to help in getting started at working with this text. Nothing in here is necessarily the best &mdash; or even a correct &mdash; way to do a given thing, but it will show you **a** way that can get you started.\n",
    "\n",
    "This is an output from a collaboration with Wellcome but I do not myself work there and I do not have expert knowledge of how their catalogue works. There are surely mistakes in what I have said and done below and I am sure that there are bugs. Patches on both counts are very welcome.\n",
    "\n",
    "See https://developers.wellcomecollection.org/docs/examples for more official and advice about working with the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using this Notebook**\n",
    "\n",
    "This notebook is split into parts to make it easier to navigate, and to skip material of less interest. You are free to skip sections of the notebook, but you will need to run all code cells.\n",
    "\n",
    "If you are using Jupyter then the simplest way to ensure this is to select `Restart Kernel and Run All Cells...` from the `Kernel` menu.\n",
    "\n",
    "Be aware that outputs may change if you re-run cells without restarting the kernel. I usually notice this when later cells change variables used in earlier cells.\n",
    "\n",
    "**Trigger Warning**\n",
    "\n",
    "This notebook engages with historic text. Opinions expressed and language used in those books may well be insensitive or even offensive from a modern perspective. I am not aware of anything problematic in the examples that I used but some elements of the notebook are quite dynamic so I am not in control of everything that you might see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting Up\n",
    "\n",
    "Now that we've covered what the notebook is and isn't, let's begin by installing the dependencies that we need and doing a little bit of setup.\n",
    "\n",
    "The next cell may take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "#You can see helper.py in the folder view on the left.\n",
    "#It contains functions that this notebook uses.\n",
    "#This lets us hide distracting details/complexity, but you can\n",
    "#look inside it if you want to know how these things are working.\n",
    "import helper\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from jsonpath_ng.ext import parse as json_parse\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from yaspin import yaspin\n",
    "import ipywidgets\n",
    "import pickle\n",
    "import lzma\n",
    "import os\n",
    "\n",
    "from IPython.display import JSON as json_display\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "seaborn.set_theme(rc={'figure.figsize':(20,10)})\n",
    "random.seed(0) #So long as this is a fixed value, operations from the random pacakge should produce the same result each time you restart the kernel.\n",
    "               #Change this to None to get more random behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Acquiring Catalogue Data\n",
    "\n",
    "We'll start this journey by using the catalogue API to search for \"typhoid\".\n",
    "\n",
    "This returns a large enough set of results to be interesting for demonstration, but a small enough set of results to be manageable.\n",
    "\n",
    "We will call the catalogue API to begin this search, and print out what it sends back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first page of catalogue results\n",
    "#Parameters that we pass to this function will be passed on to the catalogue API\n",
    "#See https://developers.wellcomecollection.org/api/catalogue more more params\n",
    "def get_cat_page(**kwargs):\n",
    "  params = {\n",
    "    'include': 'identifiers,subjects,production,languages,items', #items is the one that will allow us to find the text\n",
    "    'pageSize': 100, #number of results per page -- 100 is the maximum\n",
    "  }\n",
    "  for k, v in kwargs.items():\n",
    "    params[k.replace('_', '.')] = v #Wellcome API uses a some \".\" notation -- that won't play well with Python so we accept '_' as an alternative separator.\n",
    "  response = helper.get('https://api.wellcomecollection.org/catalogue/v2/works', params) #helper.py sits alongside this notebook\n",
    "  return response.json()\n",
    "\n",
    "page = get_cat_page(query='typhoid') #query=typhoid will be passed through to the catalogue API\n",
    "for key, value in page.items():\n",
    "  if key != 'results': #the actual results are an overwhelming amount of output so don't show them yet\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have got one page of catalogue entries back from the API. The API returns results in pages of up to 100 results at a time.\n",
    "\n",
    "When I wrote this cell, I got 12 pages of results for a total of 1105 catalogue entries (`totalResuts`). You may get different numbers if the collection has changed.\n",
    "\n",
    "The actual entries are in a `results` key. Before we look at them, let's get *all* of the results. We can write a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the first page of a set of catalogue results, get a complete list of pages\n",
    "def get_cat_pages(page):\n",
    "  if page['totalPages'] == 1:\n",
    "    return [page]\n",
    "  bar = tqdm(\n",
    "    unit = 'pages',\n",
    "    total = page['totalPages'] - 1, #-1 because we already have the first page\n",
    "    desc = 'Downloading catalogue data',\n",
    "  )\n",
    "  pages = [page]\n",
    "  while 'nextPage' in page:\n",
    "    response = helper.get(page['nextPage'])\n",
    "    page = response.json()\n",
    "    pages.append(page)\n",
    "    bar.update(1)\n",
    "  return pages\n",
    "\n",
    "#Given a list of pages, get all of the catalogue entries listed in those pages\n",
    "def cat_entries(pages):\n",
    "  entries = []\n",
    "  for page in pages:\n",
    "    entries.extend(page['results'])\n",
    "  return entries\n",
    "\n",
    "entries = cat_entries(get_cat_pages(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `get_cat_pages` has got all of the pages of catalogue entries back for us.\n",
    "\n",
    "We have then collected all of the entries into one list called `entries`.\n",
    "\n",
    "So let's see what is inside a catalogue entry &mdash; watch out, there is about to be a lot of output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_display(entries[0], expanded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Catalogue Data\n",
    "\n",
    "That's quite a lot of data, and I believe we can get more if we keep adding to the `items` parameter up in `get_cat_page`.\n",
    "\n",
    "We are interested in *text* about typhoid, so let's focus on the type of work that this entry describes &mdash; is it something written, or something else, like a drawing or a photograph?\n",
    "\n",
    "The entry is in JSON format so we can use JSONPath to look this up. The JSONPath tool that we use is [jsonpath-ng](https://pypi.org/project/jsonpath-ng/) and at times we do make use of some extensions it makes to the standard JSONPath syntax.\n",
    "\n",
    "### Formats\n",
    "\n",
    "We'll start with the \"type\" of the work. One entry in the above JSON is `workType`. The `label` and `type` look relevant. Let's examine the values that these can take across the whole collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell uses dumpCount from helper.py.\n",
    "#dumpCount takes a JSONPath query and a list of JSON objects\n",
    "#It prints text describing query results.\n",
    "\n",
    "print('workType types:')\n",
    "helper.dumpCount('$.workType.type', entries)\n",
    "print()\n",
    "print('workType labels:')\n",
    "helper.dumpCount('$.workType.label', entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm checking the `type` just to reassure myself about the data model. I'm not an expert in it, so I'm kind of figuring out how it works as I go. As I write this, the only `type` that I see is `Format`. This encourages me to believe that I don't need to worry about this value and can just think about the labels. If you need to be sure that you're using the data model correctly you might need to learn more about this.\n",
    "\n",
    "`label` shows that there are a range of types of works in our results. At the time of writing, 3/4 of the works are books and several others are of types that could reasonably have text, for example \"Archives and manuscripts\", \"Student dissertations\", \"E-books\", \"Manuscripts\", \"Journals\".\n",
    "\n",
    "Given that the text available through this API is provided by an OCR pipeline, it is only printed texts that are likely to have online text available. So let's filter down to just books, e-books and journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filters down to a list of just books, e-books and journals.\n",
    "#We cannot use JSONPath to do this because JSONPath can only check values for the\n",
    "#purpose of filtering lists, and our data appears to JSONPath code as single JSON objects\n",
    "#(re e.g. https://stackoverflow.com/a/43737750)\n",
    "def printed_entries(entries):\n",
    "  return [entry for entry in entries if entry['workType']['label'] in ['Books', 'E-books', 'Journals']]\n",
    "\n",
    "entries = printed_entries(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects\n",
    "\n",
    "Now we've filtered down our `entries` list to cover just works that seem likely to be printed.\n",
    "\n",
    "We might also be interested in the catalogued subjects of these works.\n",
    "\n",
    "Working out the catalogue subject of a work is more complicated. Works in Wellcome Collection are classified according to multiple schemes. If we look in the above JSON, we can also see that the structure there is fairly complex, involving a mixture of `subject` and `concept`.\n",
    "\n",
    "Rather than unpick all of this, let's look at a part of the structure to get a sense of how things are classified. We'll just show `subject`, and only for subjects that apply to at least 5% of the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have two columns of numbers.\n",
    "\n",
    "Before, we were dealing with formats. Each catalogue entry refers to a single physical object &mdash; a book, a journal, a picture, etc etc &mdash; and so it has only one format.\n",
    "\n",
    "Now, we are dealing with subjects and each catalogue entry may have more than one subject, so we count both \"number of entries with subject out of total entries\" and \"number of entries with subject out of total subjects counted across all entries\". Because an entry can have more than one subject it can be counted more than once.\n",
    "\n",
    "As I write, the top line of the output reads:\n",
    "`234/844 entries ( 28%);  234/2931 hits (  8%) Typhoid Fever`\n",
    "\n",
    "This means that, out of 844 catalogue entries, 234 &mdash; or 28% of all entries &mdash; have the subject \"Typhoid Fever\".\n",
    "\n",
    "Those 844 catalogue entries have a total of 2931 subjects. This includes duplicates &mdash; so if we had two catalgoue entries and they both had only the subject \"Typhoid Fever\", we would have still have 2 hits.\n",
    "\n",
    "So if we list all subjects for all entries in this corpus, 234 of them &mdash; 8% of the subjects &mdash; are \"Typhoid Fever\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive into \"Entries\" vs \"Hits\"\n",
    "\n",
    "We can unpack this with a small example. If you are not concerned about the details of this, you can fold this section away &mdash; but please run the code cells, later cells may depend on their outputs.\n",
    "\n",
    "To begin, we can look at the subjects in a sample of ten entries. We will get ten specific IDs, so that I can write these explanations with some hope that you are seeing what I saw.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The first version of this example used the first 10 works in entries, but the catalogue might change.\n",
    "#So now I get the same works by identifier, but I've changed the last two to make a better example.\n",
    "\n",
    "#Given a list of entries and list of ids, return entries with an id in the list of ids\n",
    "def get_entries_by_ids(entries, ids):\n",
    "  return [entry for entry in entries if entry['id'] in ids]\n",
    "\n",
    "#Return markdown-formatted list of some label within an entry\n",
    "def dump_labels(entries, #A list of entries\n",
    "                jsonpath, #Jsonpath to find a \"labelled thing\" (something with both the properties `label` and `id`)\n",
    "                entity_name, #A friendly name for the labelled thing\n",
    "                emphasis_pattern = ''): #A regular expression: anything matching it will be in bold\n",
    "  searcher = json_parse(jsonpath)\n",
    "  output = []\n",
    "  for entry in entries:\n",
    "    labelled_things = searcher.find(entry)\n",
    "    output.append(f'* {entry[\"title\"]} (id: {entry[\"id\"]}) [**{len(labelled_things)}** {entity_name}]')\n",
    "    for labelled_thing in labelled_things:\n",
    "      label, id = (labelled_thing.value[\"label\"], labelled_thing.value[\"id\"]);\n",
    "      x = f'{label} (id: {id})'\n",
    "      if emphasis_pattern and re.match(emphasis_pattern, label):\n",
    "        output.append(f'  * **{x}**')\n",
    "      else:\n",
    "        output.append(f'  *   {x}')\n",
    "  return Markdown('\\n'.join(output))\n",
    "\n",
    "sample = get_entries_by_ids(entries,\n",
    "                            ['bxa3fqrw','f56ccxnd','jf55amap','pw7sr9zn','q5pqqysq','qzy6ufxp','rxyt9ncw','vqhzjwd5','ab2ncfmj', 'sqwwchy7'])\n",
    "\n",
    "display(dump_labels(sample, #our small sample of entries \n",
    "                    '$.subjects[?(@.type==\"Subject\")]', #jsonpath identifying the entity we are interested in\n",
    "                    'subjects', #a nice name for the entity\n",
    "                    '^Typhoid [Ff]ever$' #any text in the output that matches this pattern will be rendered as bold\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is a total of 10 entries for printed works. These have varying numbers of subjects, totalling 18 (3 * 1 + 6 * 2 + 1 * 3 = 18).\n",
    "\n",
    "I've highlighted all uses of \"Typhoid Fever\" as a subject. You may notice that there are two different ways of identifying the general subject of \"typhoid fever\" -- one spelling fever with a capital F and one with a lower case f. These two spellings also have distinct IDs. If we wanted to find all books with this general subject then we would have to use both spellings. Even then, we would have to watch out for cases like that copy of \"On typhoid fever\", which has both spellings.\n",
    "\n",
    "You may also notice that there are two copies of William Thomson's \"On typhoid fever\". As it happens, one of these copies has two different \"Typhoid fever\" subjects, and one has only one of them.\n",
    "\n",
    "Let's now run `dumpCount` over the same ten entries, first to get the titles, then to get the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.title', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"On typhoid fever\" appears twice and all of the others appear once. \"On typhoid fever\" is therefore 20% of the sample.\n",
    "\n",
    "The numbers here are out of 10, because there are 10 entries.\n",
    "\n",
    "We only get one list of numbers because the number of titles equals the number of entries, so a second list would just be exactly the same as the first. `dumpCount` is written not to give us two lists when this happens.\n",
    "\n",
    "Now let's look at subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different numbers of entries (10) and subjects (18), as we saw above. Because of this we get two lists: the \"entries\" list calculates percentages as a proportion of the number of entries and the \"hits\" list calculates percentages as a proportion of the number of subjects.\n",
    "\n",
    "The left-hand \"entries\" column is still counting \"by entry\" &mdash; each number is out of 10, the number of entries.\n",
    "\n",
    "4/10 entries have the subject \"Typhoid Fever\", and another 3/10 have the subject \"Typhoid fever\" (upper-case vs lower-case \"f\").\n",
    "\n",
    "Looking a few cells above, we can see that the first entry (\"Typhoid fever and chronic typhoid carriers\") has the subjects \"Typhoid Fever - epidemiology\" and \"Typhoid Fever - transmission\", so it effectively appears twice in the left-hand column, once for each subject. All entries will be counted once in this column for each subject that they have. Because of this, the total of entries in the left-hand column is greater than 10 &mdash; in fact it will be 18, the total number of subjects. If we count up the percentages in this column, they will come to 180% (1 * 40 + 1 * 30 + 11 * 10 = 40 + 30 + 110 = 180).\n",
    "\n",
    "The right-hand \"hits\" column is counting \"by subject\" &mdash; each number is out of 18, the total number of subjects possessed by all of the books. Just as \"On typhoid fever\" appeared twice in our lists of titles, some subjects appear more than once when we list all of the subjects of all of the books. If we count up the percentages in this column, they will come to 100% (actually slightly higher because of rounding errors but it really is 18/18 if you add it up).\n",
    "\n",
    "Note also that the inconsistent nature of the data leads to some misrepresentation. If we normalize by case, the proportions will change a little. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of entries, returns a new list of entries with normalized subject labels\n",
    "#this is rough and ready -- it normalizes label case but ignores other attributes, such as id\n",
    "#Unlike most other functions in this code, this one returns clones of the original, not pointers to the original\n",
    "def normalize_subjects(original_entries):\n",
    "  entries = [deepcopy(entry) for entry in original_entries]\n",
    "  for entry in entries:\n",
    "    subjects = entry['subjects']\n",
    "    seen = set()\n",
    "    normalized_subjects = []\n",
    "    for subject in subjects:\n",
    "      if subject['type'] != 'Subject': continue #if it is not actually a subject, move on to the next subject\n",
    "      lowered = subject['label'].lower()\n",
    "      if lowered in seen: continue #if this work already has a subject with this label, move on to the next subject\n",
    "      subject['label'] = lowered\n",
    "      seen.add(lowered)\n",
    "      normalized_subjects.append(subject)\n",
    "    entry['subjects'] = normalized_subjects\n",
    "  return entries\n",
    "\n",
    "normalized_sample = normalize_subjects(sample) #normalized_sample is a misnomer, we are only normalizing subject label\n",
    "display(dump_labels(normalized_sample, '$.subjects[?(@.type==\"Subject\")]', 'subjects', '^typhoid fever$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general \"Typhoid Fever\" subject is now consistently \"typhoid fever\".\n",
    "\n",
    "We still have our same ten titles but now only 16 subjects (5 * 1 + 4 * 2 + 1 * 3 = 5 + 8 + 3 = 16) because \"Typhoid fever: a history\" and the first copy of \"On typhoid fever\" no longer have the same subject label listed twice with different cases.\n",
    "\n",
    "We can perform the same analysis with this slightly cleaner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', normalized_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see easily see that 60% of the works have the \"typhoid fever\" subject, which is also 38% of all of the subjects covered.\n",
    "\n",
    "There is more we could do to clean this data, and to make sure of our assumptions about the data model. For example, I am assuming that one work cannot have two completely identical subjects. But let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "You might recall that there is more than one way of talking about classifications in the data model, so let's take a quick look at that back in our full set of entries describing printed works. We'll again look at \"subjects\" applying to at least 5% of books in our `entries`, but now we'll look at \"concepts\" too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between \"Typhoid Fever\" and \"Typhoid fever\" is back with us because we only lower-cased subject labels in a sample that we copied out of `entries`. \n",
    "\n",
    "Still, this gives us an impression of the state of our corpus. At time of writing, 9% of these entries have no subjects and 10% have no concepts.\n",
    "\n",
    "We can also see that the subjects and concepts are quite similar. The concepts look maybe finer-grained, but I'm mostly guessing.\n",
    "\n",
    "If you want to see more of the subjects/concepts in the collection, make the number at the end of each `dumpCount` call smaller, or remove it to see all of them. There will be a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works Held by Wellcome\n",
    "\n",
    "One difficulty may be that Wellcome's catalogue includes texts belonging to other collections. These could be classified in different ways.\n",
    "\n",
    "So let's assume that we are interested in searching works actually held by Wellcome itself and limit down to them.\n",
    "\n",
    "The way that was suggested to me to do this was to look for works held on either open shelves or in closed stores. This seems to make sense. Perhaps it needs a tweak for purely digital works such as E-books, but at the time I was dealing with works that were old enough that I didn't need to think about that. For purposes of this notebook we will continue not to worry about that, so let's filter down our `entries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All availability ids within entries:')\n",
    "helper.dumpCount('$.availabilities[*].id', entries)\n",
    "print()\n",
    "\n",
    "#Given a list of entries, return those that \"appear to be held by Wellcome\"\n",
    "def wellcome_entries(entries):\n",
    "  open_searcher   = json_parse(\"$.availabilities[?(@.id=='open-shelves')].id\")\n",
    "  closed_searcher = json_parse(\"$.availabilities[?(@.id=='closed-stores')].id\")\n",
    "  return [entry for entry in entries if len(open_searcher.find(entry)) > 0 or len(closed_searcher.find(entry)) > 0]\n",
    "\n",
    "total_entries = len(entries)\n",
    "entries = wellcome_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} printed works are available in closed and/or open stores (therefore held by Wellcome itself)')\n",
    "total_entries = len(entries)\n",
    "print('These break down as:')\n",
    "helper.dumpCount('$.workType.label', entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `entries` uses the availabilities `online`, `closed-stores`, and `open-shelves`.\n",
    "\n",
    "484 (at time of writing) `entries` are held by Wellcome itself, so we have replaced our old `entries` list with just these ones. Most of these entries describe books.\n",
    "\n",
    "You may find that the total of `closed-stores` and `open-shelves` entries is greater than the number of entries held by Wellcome. I assume that this is due to some entries having copies both on open shelves and in closed stores, and/or due to multiple copies of an entry in a single location.\n",
    "\n",
    "Now that we are looking at something like \"just works held by Wellcome\", we can look again at concepts and subjects to see what the coverage is like for the particular works that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)\n",
    "\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time of writing, the subjects `Typhoid Fever - epidemiology` and `Typhoid Fever` are both well represented among printed works held at Wellcome. 30% of entries have the subject `Typhoid Fever` and 28% have the subject `Typhoid Fever - epidemiology`. Up to 58% of the whole corpus thefore has one or other of these subjects &mdash; but some works might have both subjects, so the real proportion may well be lower.\n",
    "\n",
    "A very large 76% of entries have the concept `Typhoid Fever` at time of writing &mdash; a proportion that is even more significant given that 15% of entries have no concept at all.\n",
    "\n",
    "As there are still entries missing subjects and concepts, lack of classification information is not explained only by a work not being held at Wellcome (or we have done something wrong).\n",
    "\n",
    "We could keep digging, but let's just define a couple of functions for getting at subjects and concepts and then move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get subject labels from an entry\n",
    "def subjects(entry):\n",
    "  #sorted just because I like repeatability\n",
    "  return sorted(set([subject.lower() for subject in helper.list_by_jsonpath('$.subjects[?(@.type==\"Subject\")].label', [entry])]))\n",
    "\n",
    "#Get concept labels from an entry\n",
    "def concepts(entry):\n",
    "  #sorted just because I like repeatability\n",
    "  return sorted(set([concept.lower() for concept in helper.list_by_jsonpath('$.subjects..*[?(@.type==\"Concept\")].label', [entry])]))\n",
    "\n",
    "print('Quick check -- subjects of first entry:')\n",
    "print('\\n'.join(subjects(entries[0])))\n",
    "\n",
    "print()\n",
    "print('Quick check -- concepts of first entry:')\n",
    "print('\\n'.join(concepts(entries[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates\n",
    "\n",
    "Before we move on to looking at text, let's also get a sense of when works were published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dates of printed Wellcome works, by frequency (min 1%)\")\n",
    "helper.dumpCount('$.production[*].dates[*].label', entries, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this lists only a few dates, it is enough for us to see that the date format is not completely consistent (at least at time of writing). Sometimes we get a year, sometimes we get a year inside square brackets.\n",
    "\n",
    "Let's see how many of the dates do not consist entirely of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = helper.list_by_jsonpath('$.production[*].dates[*].label', entries)\n",
    "not_a_number = Counter([x for x in dates if not x.isnumeric()])\n",
    "print('; '.join(sorted(not_a_number.keys())))\n",
    "print()\n",
    "print(f'Total \"bad\" dates: {not_a_number.total()}/{len(dates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time of writing, there are quite a few \"square brackets\" cases, but also date ranges, copyright symbols, question marks and occasionally snippets of text. 28% of dates are not numbers, which seems like a lot.\n",
    "\n",
    "Let's try just stripping out square brackets: this discards some information that might be important but for now we just want a rough sense of the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [x.strip('[]') for x in dates]\n",
    "not_a_number = Counter([x for x in dates if not x.isnumeric()])\n",
    "print('; '.join(sorted(not_a_number.keys())))\n",
    "print()\n",
    "print(f'Total \"bad\" dates: {len(not_a_number)}/{len(dates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time of writing, this reduces the \"bad date\" proportion to 8%.\n",
    "\n",
    "We could do more, but let's carry on with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dates of printed Wellcome works, roughly chronologically ordered, discarding non-numbers:\")\n",
    "dates = sorted([int(x) for x in dates if x.isnumeric()])\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a sense of the corpus, at time of writing the works span from 1762 to the modern day.\n",
    "\n",
    "I also see 18 and 19. These are questionable numbers for dates. These could refer to very early works but my first guess is that these are the first two digits of a century.\n",
    "\n",
    "Let's just drop those two numbers and then look at this data in a more visual form. We'll start by writing a function that pulls together the above rough date-handling techniques, then use that to get our dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work out the date for an entry, doing some very rough cleaning\n",
    "def date(entry):\n",
    "  dates = helper.list_by_jsonpath('$.production[*].dates[*].label', [entry])\n",
    "  if len(dates) == 0: return None\n",
    "  date = dates[0] #we won't worry about dealing with more than one date\n",
    "  date = date.strip('[]')\n",
    "  if not date.isnumeric(): return None\n",
    "  date = int(date)\n",
    "  if date > 1000 and date < 2100: return date\n",
    "  return None\n",
    "\n",
    "#recompute the dates\n",
    "dates = [date(entry) for entry in entries] #get date from all entries\n",
    "dates = sorted([date for date in dates if date]) #sort, filtering out the Nones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate some charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_dates = Counter(dates)\n",
    "total = 0\n",
    "cumulative = {}\n",
    "for year in set(dates):\n",
    "  total += counted_dates[year]\n",
    "  cumulative[year] = total\n",
    "\n",
    "xlim = (helper.down(dates[0], 50), helper.up(dates[-1], 50))\n",
    "xticks = range(xlim[0], xlim[1] + 1, 25)\n",
    "\n",
    "ylim = (0, helper.up(counted_dates.most_common(1)[0][1], 2))\n",
    "ax = seaborn.scatterplot(counted_dates)\n",
    "ax.set(\n",
    "  title = 'Publications per year',\n",
    "  xlabel = 'Year',\n",
    "  ylabel = 'Works',\n",
    "  ylim = ylim,\n",
    "  yticks = range(ylim[0], ylim[1] + 1, 2),\n",
    "  xlim = xlim,\n",
    "  xticks = xticks,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "ylim = (0, helper.up(total, 50))\n",
    "ax = seaborn.lineplot(cumulative)\n",
    "ax.set(\n",
    "  title = 'Cumulative publications per year',\n",
    "  xlabel = 'Year',\n",
    "  ylabel = 'Cumulative works',\n",
    "  ylim = ylim,\n",
    "  yticks = range(ylim[0], ylim[1] + 1, 25),\n",
    "  xlim = xlim,\n",
    "  xticks = xticks,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I write, these charts show that Wellcome's own collection of printed works that are returned for a search on 'typhoid' were published mainly between the late 1800s and early 1900s -- discounting all the ones with dates that were not easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages \n",
    "\n",
    "OK, let's get on to actually accessing the text of our works. We'll keep the ones with funny dates, but quickly throw away the ones that are not in English, so that we're working with a single language.\n",
    "\n",
    "We'll do this by finding all ids belonging to books that have a non-English language, and then throwing away those ids. We do it like this because some books may have more than one language listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return all entries that do not have any non-English languages listed\n",
    "#(Avoiding getting such texts from the API in the first place is probably a better way to deal with this)\n",
    "def english_entries(entries):\n",
    "  non_english_ids = set(helper.list_by_jsonpath('$.languages[?(id!=\"eng\")].`parent`.`parent`.id', entries))\n",
    "  return [entry for entry in entries if not entry['id'] in non_english_ids]\n",
    "\n",
    "entries = english_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} printed works held by Wellcome are in English (or at least are not labelled as non-English)')\n",
    "total_entries = len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Accessing Text\n",
    "\n",
    "Now we have explored what the catalogue can tell us a little and learned a little bit about the texts in the corpus.\n",
    "\n",
    "There is certainly more you could do to get a coherent corpus, for example looking at a particular place and/or time. But let's work with the corpus that we now have: results from a search for \"typhoid\", from any time, but only in English.\n",
    "\n",
    "The next step is to find out which ones actually have digitised text available. The text is made available through a IIIF manifest, so we go looking for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of entries, return all those that appear to have a IIIF manifest\n",
    "#TODO: If there are multiple URLs, do I get multiple entries here?\n",
    "#      That would mean more than one manifest for a work, which hopefully doesn't happen.\n",
    "def iiif_entries(entries):\n",
    "  #This bit of JSONPath finds every member of the `items` array that ultimately turns out to contain a IIIF location.\n",
    "  #It then gets the grandparent of that iten, which should be the data structure that we started with.\n",
    "  #In this way, we can filter our list of printed Wellcome works to just those for which we have OCR'd text.\n",
    "  return helper.list_by_jsonpath('$.items[?(@.locations[*].locationType.id=\"iiif-presentation\")].`parent`.`parent`', entries)\n",
    "\n",
    "entries = iiif_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} entries have a IIIF manifest.')\n",
    "total_entries = len(entries)\n",
    "\n",
    "#Return a simple text description from an entry\n",
    "#Very much assuming single-entry lists here\n",
    "def description(entry):\n",
    "  date = helper.list_by_jsonpath('$.production[*].dates[*].label', [entry])\n",
    "  if len(date) > 0:\n",
    "    date = date[0]\n",
    "  else:\n",
    "    date = '<no date>'\n",
    "  return (f'id {entry[\"id\"] if \"id\" in entry else \"<no id>\"}'\n",
    "           ' -- '\n",
    "          f'{entry[\"title\"] if \"title\" in entry else \"<no title>\"}, '\n",
    "          f'published {date}')\n",
    "\n",
    "print('The first of these in the list is ' + description(entries[0]))\n",
    "\n",
    "#Get the URL of the IIIF manifest describing an entry\n",
    "def iiif_manifest_url(entry):\n",
    "  return helper.expect_one(helper.list_by_jsonpath('$.items[*].locations[?(@.locationType.id=\"iiif-presentation\")].url', [entry]))\n",
    "\n",
    "print('Its IIIF manifest is at', iiif_manifest_url(entries[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to click on that manifest and take a look. Next we will check the entries with IIIF manifests to make sure that they do actually have plain text available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a IIIF manifest, given its URL\n",
    "def get_iiif_manifest(url):\n",
    "  return helper.get(url).json()\n",
    "\n",
    "#Given a list of entries, return all those that appear to have OCR text actually available\n",
    "def ocr_entries(entries):\n",
    "  bar = tqdm(\n",
    "    unit = 'works',\n",
    "    total = len(entries),\n",
    "    desc = 'Checking for text of works',\n",
    "  )\n",
    "  new_entries = []\n",
    "  for entry in entries:\n",
    "    try:\n",
    "      manifest = get_iiif_manifest(iiif_manifest_url(entry))\n",
    "      if len(helper.list_by_jsonpath('$.sequences[*].rendering[?(@.format=\"text/plain\")].@id', [manifest])) >= 1:\n",
    "        new_entries.append(entry)\n",
    "    except Exception:\n",
    "      print(f'Failed to get single manifest for {description(entry)}')\n",
    "    bar.update(1)\n",
    "  return new_entries\n",
    "\n",
    "entries = ocr_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} entries have plain text available.')\n",
    "total_entries = len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the URL where the plain text of a work can be found\n",
    "def text_url(manifest):\n",
    "  return helper.expect_one(helper.list_by_jsonpath('$.sequences[*].rendering[?(@.format=\"text/plain\")].@id', [manifest]))\n",
    "\n",
    "print('The first entry with plain text available is', description(entries[0]))\n",
    "print('Its manifest is at', iiif_manifest_url(entries[0]))\n",
    "print('And the plain text is at', text_url(get_iiif_manifest(iiif_manifest_url(entries[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there should only be one plain text URL and you can click on it if you would like to take a look. As a potentially large body of unformatted text, it may not be very easy to read.\n",
    "\n",
    "Now let's get that text so that we can do something with it. We'll print out some random sentences so that we can see it has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the plain text of a work, given the URL of that plain text\n",
    "#(In other words, return the content of an arbitrary URL as plain text)\n",
    "def get_plain_text(url):    \n",
    "  response = helper.get(url)\n",
    "  return response.text\n",
    "\n",
    "text = get_plain_text(text_url(get_iiif_manifest(iiif_manifest_url(entries[0]))))\n",
    "sentences = [x + '.' for x in text.split('.')] #an approximation to a list of sentences from the book\n",
    "print(\"Some randomly-selected sentences, just to prove that we have got some text from a work:\")\n",
    "print()\n",
    "print()\n",
    "display(Markdown(helper.markdown_n_list(random.sample(sentences, k=10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some analysis. I'm going to use a specific id to get an example text &mdash; there's a good chance it'll be the same one we just got, but it might not be.\n",
    "\n",
    "We'll start by getting that text, in the same way that we did above, but all in one cell this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the example that I am using\n",
    "sample = helper.expect_one([entry for entry in entries if entry['id']=='kbspden9'])\n",
    "print(f'The example work is {description(sample)}')\n",
    "\n",
    "#Given an entry (rather than a URL), get that entry's plain text\n",
    "def get_plain_text_from_entry(entry):\n",
    "  return get_plain_text(text_url(get_iiif_manifest(iiif_manifest_url(entry))))\n",
    "\n",
    "text = get_plain_text_from_entry(sample)\n",
    "sentences = [x + '.' for x in text.split('.')] #an approximation to a list of sentences from the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analyzing Text\n",
    "\n",
    "Now we shall generate a table showing the frequencies of the most used words in the work's text, and we'll generate a word cloud showing the same information.\n",
    "\n",
    "We will ignore common words that do not convey much information (\"stop words\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you would like to generate a cloud with different settings, adjust some of these parameters\n",
    "wc = WordCloud(width=1024, height=800, min_font_size=20, \n",
    "               margin=75, #space between words\n",
    "               background_color='white', color_func=lambda *args, **kwargs: 'black', #white background, black text\n",
    "               max_words=10,\n",
    "#               stopwords=set() #uncomment this line to prevent stopword removal, or populate the set to give your own stopwords.\n",
    "                                #The default stopwords are here: https://github.com/amueller/word_cloud/blob/main/wordcloud/stopwords.\n",
    ")\n",
    "freq = wc.process_text(text)\n",
    "wc.generate_from_frequencies(freq) #we can just to wc.generate(example_text), but this way we get the frequency table, too\n",
    "display(pd.DataFrame(Counter(freq).most_common()[0:wc.max_words], columns=['word','count']).set_index('word'))\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"fever\" is the most frequently occuring word in the text, with \"case\", \"one\" and \"â\" also coming up a lot.\n",
    "\n",
    "It might be interesting to compare the table of frequencies with the cloud. Word clouds are nice and intuitive, but do these sizes really reflect the proportions in the table? If not, how much does it matter, and does that depend on what you are using it for?\n",
    "\n",
    "Anyway, that \"â\" all by itself curious. It might well be a transcription error, let's look for it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_circumflex_mentions = [x for x in sentences if 'â' in x.lower()]\n",
    "print(f'{len(a_circumflex_mentions)}/{len(sentences)} sentences contain \"â\"')\n",
    "print('\\nHere are the first 10 of them:\\n')\n",
    "display(Markdown(helper.markdown_n_list(a_circumflex_mentions[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of these seem are likely to be apostrophes or quote marks, as in \"Budd's original essay\" or \"the 'Lancet,'\". Others are harder to interpret. Using the URLs generated by the next cell in Wellcome's catalogue. That will give us access to a IIIF view of the work which we can use to search the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Catalogue:      https://wellcomecollection.org/search/works?query=' + sample['id'])\n",
    "print('Digitised work: https://wellcomecollection.org/works/' + sample['id'] + '/items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the latter URL to search for some of the above sentences, I see that 'â' seems to show up as either an apostrophe/quote mark or an em-dash. So I'm going to assume it is getting rendered in place of some kinds of punctuation. Let's see if we can clean this out of our text, and what that does to the word cloud.\n",
    "\n",
    "(I'm sure there are better ways to view the above sentences in the original text. Patches welcome.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('â', '')\n",
    "freq = wc.process_text(text)\n",
    "wc.generate_from_frequencies(freq) #we can just to wc.generate(example_text), but this way we get the frequency table, too\n",
    "display(pd.DataFrame(Counter(freq).most_common()[0:10], columns=['word','count']).set_index('word'))\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has allowed \"time\" to make it onto our list of top words.\n",
    "\n",
    "There is more we can do &mdash; for example, \"may\" seems quite meaningless unless, perhaps, it is referring to the month.\n",
    "\n",
    "But let's move on. We'll assume that the text is clean enough with our one little \"a circumflex\" clean up and see wht we can with topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Topic Modelling\n",
    "\n",
    "We'll use `gensim` to create a topic model of our corpus and `pyLDAVis` to visualise that.\n",
    "\n",
    "We are working with a single text here, so we might not expect the topic model to be all that interesting, but we can see how to create and explore it.\n",
    "\n",
    "We are getting outside of my comfort zone here &mdash; I am not an expert in topic modelling, gensim or pyLDAVis so don't take my explanations too seriously. I'm trying to point you towards how to do things, not to explain them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following https://neptune.ai/blog/pyldavis-topic-modeling-exploration-tool\n",
    "\n",
    "#Rebuild the text into the form that gensim expects\n",
    "#Don't worry too much about this bit, there is a more sensible example of setting up gensim topic models below\n",
    "text = []\n",
    "for word, count in freq.items():\n",
    "  text.extend([word] * count)\n",
    "id2word = Dictionary([text])\n",
    "corpus = [id2word.doc2bow(text)]\n",
    "#pprint(id2word)\n",
    "\n",
    "lda_model = LdaModel(corpus = corpus, id2word = id2word,\n",
    "                     num_topics = 3,\n",
    "                     random_state = 0,\n",
    "                     chunksize = 1, #Number of documents per training chunk. We actually only have 1 document.\n",
    "                     alpha = 'auto',\n",
    "                     eta = 'auto',\n",
    "                     per_word_topics = True,\n",
    ")\n",
    "#doc_lda = lda_model[corpus]\n",
    "#pprint(lda_model.print_topics()) #uncomment this to get a sense of what the topics \"really are\" (a list of words and weights)\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each circle on the left is a topic. The size of the circles indicates the extent to which that topic appears in the corpus.\n",
    "\n",
    "The distance between topics should give a sense of how similar or different they are.\n",
    "\n",
    "The lambda ($\\lambda$) parameter at top right adjusts the weight given to term frequency vs how reflective a term is of a particular topic. Lower lambda values might make it easier to interpret the topics.\n",
    "\n",
    "The bars on the right show frequency of word in work. If you hover over one of the circles then you'll see the word in the relevant topic as red bars. That lambda parameter plays into what you see here. Try comparing the larger circles with different lambda values, such as 0.1.\n",
    "\n",
    "If you change the `num_topics` parameter and re-run the above cell then you will see a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Full Corpus\n",
    "\n",
    "Now let's collect all of the text for our corpus (printed works about typhoid held at Wellcome, not in non-English). While we are at, we will make some simpler data structures so that we can easily get at the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of works, return a data structure giving easy access to some key information, including its text\n",
    "def get_works(entries):\n",
    "  bar = tqdm(\n",
    "    unit = 'works',\n",
    "    total = len(entries),\n",
    "    desc = 'Downloading text of works',\n",
    "  )\n",
    "\n",
    "  works = []\n",
    "  for entry in entries:\n",
    "    text = ''\n",
    "    try:\n",
    "      text = get_plain_text_from_entry(entry)\n",
    "    except Exception as e:\n",
    "      print(f'Unable to get text for {description(entry)}')\n",
    "      #print(f'Unable to get text for {description(entry)} [cause: {e}]')\n",
    "      bar.update(1)\n",
    "      continue #do not create an entry if there is no text to work with\n",
    "\n",
    "    works.append({\n",
    "      'id': entry['id'],\n",
    "      'title': entry['title'],\n",
    "      'date': date(entry),\n",
    "      'subjects': subjects(entry),\n",
    "      'concepts': concepts(entry),\n",
    "      'text': text,\n",
    "    })\n",
    "\n",
    "    bar.update(1)\n",
    "  return works\n",
    "\n",
    "works = get_works(entries)\n",
    "print(f'\\nGot text for {len(works)}/{len(entries)} works.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already filtered out entries where the work had no text, so we should have got text for all of our remaining entries here.\n",
    "\n",
    "Let's take a look at one of the data structures in our new `works` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a work, pretty-print it with markdown\n",
    "def pp_work(work):\n",
    "  output = []\n",
    "  for key, value in works[0].items():\n",
    "    key = key.capitalize()\n",
    "    if key == 'Text': #just show the first 1000 characters\n",
    "      output.append(f'* **{key} (first 1000 characters):** {value[0:1000]}')\n",
    "    elif key == 'Tokens': #just show the first 100 tokens (will only happen if we re-run this cell after creating some tokens)\n",
    "      output.append(f'* **{key} (first 100):** {value[0:100]}')\n",
    "    else:\n",
    "      output.append(f'* **{key}:** {value}')\n",
    "  display(Markdown('\\n'.join(output)))\n",
    "    \n",
    "pp_work(works[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now we need to clean/preprocess the text. We won't get too deep into this, but we will do a few things:\n",
    "\n",
    "1. Remove stopwords (low-information words like \"and\", \"but\")\n",
    "2. Stem: Chop off certain suffixes, so that different forms of the same word are treated as the same word.\n",
    "3. Tokenize: split the text into individual tokens. In this case, I believe into words.\n",
    "4. Lowercase: Put all words into lower-case so that capitalized and uncapitalized words are treated as the same word.\n",
    "5. Drop short tokens: Short tokens are dropped. This will, for example, get rid of single-letter words and hopefully some OCR errors.\n",
    "\n",
    "Stemming is a bit of a blunt tool -- just chopping off certain suffixes can lead to treating different words as being the same. It can also make stop words seem to reappear, but now the \"stop word\" is actually a stem of some more meaningful word.\n",
    "Lemmatizing is a more sophisticated alternative to stemming but it requires more compute so we will not try it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of works, preprocess the text in each work and store the preprocessed text in a 'tokens' key\n",
    "def tokenize(works):\n",
    "  bar = tqdm(\n",
    "    unit = 'works',\n",
    "    total = len(works),\n",
    "    desc = 'Preparing text of works',\n",
    "  )\n",
    "\n",
    "  token_count = 0\n",
    "  tokenized_works = []\n",
    "  for work in works:\n",
    "    destopped = gensim.parsing.remove_stopwords(work['text'])\n",
    "    stemmed = gensim.parsing.stem_text(destopped)\n",
    "    all_tokens = list(gensim.utils.tokenize(stemmed, True)) #True imposes lower case\n",
    "    tokens = [token for token in all_tokens if len(token) >= 3] #Drop very short tokens\n",
    "    if(len(tokens)): #drop documents that are empty after preprocessing\n",
    "      work['tokens'] = tokens\n",
    "      tokenized_works.append(work)\n",
    "    token_count += len(tokens)\n",
    "    bar.update(1)\n",
    "\n",
    "  print(f'Generated {token_count:,} tokens')\n",
    "  return tokenized_works\n",
    "\n",
    "total_works = len(works)\n",
    "works = tokenize(works)\n",
    "print(f'{len(works)}/{total_works} works preprocessed for model')\n",
    "total_works = len(works)\n",
    "\n",
    "#Defined as a function just to avoid polluting the global namespace any further with short-term variables\n",
    "def report_tokens(work):\n",
    "  early_tokens = work['tokens'][0:100]\n",
    "  print()\n",
    "  print(f'First {len(early_tokens)} tokens:')\n",
    "  print(' '.join(early_tokens))\n",
    "\n",
    "  unprocessed_early_tokens = list(gensim.utils.tokenize(work['text']))[0:150]\n",
    "  print()\n",
    "  print(f'For comparison, the first {len(unprocessed_early_tokens)} tokens without preprocessing are:')\n",
    "  print(' '.join(unprocessed_early_tokens))\n",
    "\n",
    "report_tokens(works[0])\n",
    "\n",
    "#Uncomment for testing the later \"redownload from Wellcome\" option. Typhoid corpus should be identical\n",
    "#with open('original_typhoid.p', 'wb') as f:\n",
    "#  pickle.dump(works, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the Full Corpus\n",
    "\n",
    "Now we will generate a new topic model from this larger corpus. The next cell may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of works and an optional number of topics, generate a model with that number of topics\n",
    "def model(works, num_topics = 10):\n",
    "  id2word = Dictionary([work['tokens'] for work in works])\n",
    "  corpus = [id2word.doc2bow(work['tokens']) for work in works]\n",
    "  model = LdaModel(corpus = corpus, id2word = id2word,\n",
    "                   num_topics = num_topics, #Number of topics to generate\n",
    "                   random_state = 0,        #Random seed. Keeping this number the same should generate the same topics each time. Changing it should change them.\n",
    "                   chunksize = len(corpus), #Number of documents per training chunk. We are dealing with a small number of docs, so may as well use all of them\n",
    "                   alpha = 'auto',          #Hyperparameter -- we won't get into these\n",
    "                   eta = 'auto',            #Another hyperparameter\n",
    "                   per_word_topics = True,  #Generates some additional information\n",
    "  )\n",
    "  return id2word, corpus, model\n",
    "\n",
    "id2word = None\n",
    "corpus = None\n",
    "lda_model = None\n",
    "\n",
    "with yaspin(text=\"Generating topic model. May take a few minutes.\", timer = True) as sp:\n",
    "  id2word, corpus, lda_model = model(works)\n",
    "  print(f'\\nTook {int(sp.elapsed_time)}s to generate model.')\n",
    "\n",
    "\n",
    "with yaspin(text='Displaying topic model'):\n",
    "  display(pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, feel free to explore this topic model. You might again want to set $\\lambda$ to a lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the topic distribution per document which &mdash; assuming that the calculation I am doing here makes sense, which it may or may not &mdash; should give a sense of which are the main topics for a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a simple interface\n",
    "#Generate dropdown list of all available documents\n",
    "#Selecting a document displays a bar graph showing importance of topic in document\n",
    "def document_topics_bar():\n",
    "  output2 = ipywidgets.Output()\n",
    "\n",
    "  def doc_topics_bar(output, doc_index):\n",
    "    output.clear_output()\n",
    "    with(output):\n",
    "      doc_topics = {'Topic #': [], 'Probability': []}\n",
    "      for topic, probability in lda_model.get_document_topics(corpus[doc_index]):\n",
    "        doc_topics['Topic #'].append(topic)\n",
    "        doc_topics['Probability'].append(probability)\n",
    "      ax = seaborn.barplot(doc_topics, x='Topic #',y='Probability')\n",
    "      ax.set(\n",
    "        title = f'Main topics of \"{works[doc_index][\"title\"]}\" ({works[doc_index][\"id\"]})',\n",
    "        ylim = (0, 1),\n",
    "      )\n",
    "      plt.show()\n",
    "\n",
    "  dd = ipywidgets.Dropdown(\n",
    "    options = [(f'{work[\"title\"]} ({work[\"date\"]}) [{work[\"id\"]}]', index) for index, work in enumerate(works)],\n",
    "    value = 0,\n",
    "    description = 'Work',\n",
    "    layout={'width': '90%'},\n",
    "  )\n",
    "\n",
    "  def switcher(change):\n",
    "    doc_topics_bar(output2, change['new'])\n",
    "\n",
    "  switcher({'new': 0})\n",
    "  dd.observe(switcher, names = 'value')\n",
    "  display(dd, output2)\n",
    "\n",
    "document_topics_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each bar should correspond to a topic number. This should correspond to the topic numbers in pyLDAVis.\n",
    "\n",
    "Topics have to make a minimum contribution to a document to appear, so you generally won't see a bar for all of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Corpora\n",
    "\n",
    "Finally, we use the tools that we build above to create a simple interface that lets us inspect a few different corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Would really like to sort out the scoping here. Setting things to global generally doesn't end well.\n",
    "#TODO: Would also be good to get the bits of the UI to interact nicely.\n",
    "\n",
    "#Define some corpora to fetch\n",
    "#Note that the Wellcome API can do some filtering for us\n",
    "#There are a couple of examples of that here\n",
    "#See https://developers.wellcomecollection.org/api/catalogue#tag/Works/operation/getWorks for more\n",
    "#Here, params is used to specify params initially passed to the Wellcome API\n",
    "#max_topics is a guess as to the highest number of topics that we can cope with generating for a corpus of this size\n",
    "#max_topics was set by trial and error, your mileage may vary\n",
    "#max_topics limiting factor seems to be what pyLDAVis can cope with\n",
    "CORPORA = {\n",
    "  'Typhoid': {\n",
    "    'params': {'query': 'typhoid'}, #Should result in the same corpus that we built above\n",
    "    'max_topics': 100,\n",
    "  },\n",
    "  'Fever': {\n",
    "    'params': {'query': 'fever'},\n",
    "    'max_topics': 30,\n",
    "  },\n",
    "  '1780-1789': {\n",
    "    'params': {'production.dates.from': '1780-01-01', 'production.dates.to': '1789-12-31'}, #Can do date filter direct in API\n",
    "    'max_topics': 15,\n",
    "  },\n",
    "  '1880-1882': {\n",
    "    'params': {'production.dates.from': '1880-01-01', 'production.dates.to': '1882-12-31'}, #Another date example -- narrower window as generally expect to find more works for more recent dates\n",
    "    'max_topics': 10,\n",
    "  },\n",
    "  'Melancholy': {\n",
    "    'params': {'subjects.label': 'Melancholy'}, #Can do subject filter direct in API -- but this only gets one document back, so I'm doing something wrong\n",
    "    'max_topics': 100,\n",
    "  },\n",
    "}\n",
    "\n",
    "#Dropdown listing available corpora\n",
    "corpus_dd = ipywidgets.Dropdown(\n",
    "  options = list(CORPORA.keys()),\n",
    "  description = 'Corpus',\n",
    ")\n",
    "\n",
    "#Dropdown listing available views (pyLDAVis or the bar chart)\n",
    "view_dd = ipywidgets.Dropdown(\n",
    "  options = ['pyLDAVis', 'Document Topics'],\n",
    "  value = 'Document Topics',\n",
    "  description = 'View',\n",
    ")\n",
    "output = ipywidgets.Output()\n",
    "\n",
    "#Generate a new topic model based on the given works and topic count\n",
    "def remodel(works, topic_count):\n",
    "  global id2word\n",
    "  global corpus\n",
    "  global lda_model\n",
    "  output.clear_output()\n",
    "  with yaspin(text=f\"Generating topic model with {topic_count} topics. May take a few minutes.\", timer = True) as sp:\n",
    "    id2word, corpus, lda_model = model(works, topic_count)\n",
    "    print(f'\\nTook {int(sp.elapsed_time)}s to generate model.')\n",
    "  output.clear_output()\n",
    "  if view_dd.value == 'Document Topics':\n",
    "    with output:\n",
    "      document_topics_bar()\n",
    "  else:\n",
    "    view_dd.value = 'Document Topics'\n",
    "\n",
    "#Get a corpus from the Wellcome API and generate a model from it\n",
    "def reload(corpus, topic_count):\n",
    "  global works\n",
    "  def get_entries(**kwargs):\n",
    "    return cat_entries(get_cat_pages(get_cat_page(**kwargs)))\n",
    "  def common_filters(entries):\n",
    "    return ocr_entries(english_entries(wellcome_entries(printed_entries(entries))))\n",
    "  def tokens(entries):\n",
    "    return tokenize(get_works(entries))\n",
    "\n",
    "  with output:\n",
    "    print('Any works with missing text will be listed as \"Failed to get single manifest\" or \"Unable to get text\", there may be quite a few...')\n",
    "    print('Later progress bars will appear under this list, if nothing seems to be happening then you can scroll down to check')\n",
    "    if corpus in CORPORA.keys():\n",
    "      try:\n",
    "        works = tokens(common_filters(get_entries(**CORPORA[corpus]['params'])))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "    else:\n",
    "      return\n",
    "\n",
    "  with yaspin(text = 'Writing cache                                                       '):\n",
    "    with lzma.open(f'{corpus}.p.xz', 'wb') as f:\n",
    "      pickle.dump(works, f)\n",
    "\n",
    "  remodel(works, topic_count)\n",
    "\n",
    "#Get a locally-cached corpus and generate a model from it\n",
    "def load(corpus, topic_count):\n",
    "  global works\n",
    "  with yaspin(text = 'Loading cache                                                       '):\n",
    "    with lzma.open(f'{corpus}.p.xz', 'rb') as f:\n",
    "      works = pickle.load(f)\n",
    "  remodel(works, topic_count)\n",
    "\n",
    "#Interface for switching between corpora\n",
    "#Includes a slider for topic count and an improvised dialog box for choosing between cached corpora and reloading them from Wellcome API\n",
    "def corpus_switcher(change):\n",
    "  corpus = change['new']\n",
    "\n",
    "  topic_count_slider = ipywidgets.IntSlider(\n",
    "    value = 10,\n",
    "    min = 2,\n",
    "    max = CORPORA[corpus]['max_topics'],\n",
    "    step = 1,\n",
    "    description = f'# Topics (2-{CORPORA[corpus][\"max_topics\"]})',\n",
    "    tooltip = f'Set number of topics (2-{CORPORA[corpus][\"max_topics\"]})',\n",
    "    continuous_update = False,\n",
    "    orientation = 'horizontal',\n",
    "    readout = True,\n",
    "    readout_format = 'd',\n",
    "    layout = {'width': '75%'}\n",
    "  )\n",
    "\n",
    "  def handle_click(button):\n",
    "    output.clear_output()\n",
    "\n",
    "    #Hacky -- look at first character in button text to decide what to do\n",
    "    if button.description[0] == 'L':\n",
    "      load(corpus, topic_count_slider.value)\n",
    "    elif button.description[0] == 'R' or button.description[0] == 'D':\n",
    "      reload(corpus, topic_count_slider.value)\n",
    "\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    display(topic_count_slider)\n",
    "    if(os.path.isfile(f'{corpus}.p.xz')):\n",
    "      loadButton   = ipywidgets.Button(description = 'Load from cache (faster)', layout = {'width': '30%'})\n",
    "      reloadButton = ipywidgets.Button(description = 'Reload from Wellcome (slower)', layout = {'width': '30%'})\n",
    "      loadButton.on_click(handle_click)\n",
    "      reloadButton.on_click(handle_click)\n",
    "      print('Cache detected: using this will save on downloading and preprocessing.')\n",
    "      print('You still have to wait for the modelling.')\n",
    "      display(ipywidgets.HBox([loadButton, reloadButton]))\n",
    "    else:\n",
    "      button = ipywidgets.Button(description = 'Download corpus and generate model', layout = {'width': '30%'})\n",
    "      button.on_click(handle_click)\n",
    "      display(button)\n",
    "\n",
    "#Generate pyLDAVis interface for model currently described by the globals lda_model, corpus and id2word\n",
    "def pyldavis():\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    #The extra spaces in the yaspin text are a disgusting hack to dodge a display update bug\n",
    "    with yaspin(text='Displaying topic model (may take a little while)                                         '):\n",
    "      display(pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word))\n",
    "\n",
    "#Switch between the model-viewing interfaces\n",
    "def view_switcher(change):\n",
    "  value = change['new']\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    if value == 'pyLDAVis': pyldavis()\n",
    "    elif value == 'Document Topics': document_topics_bar()\n",
    "\n",
    "corpus_dd.observe(corpus_switcher, names = 'value')\n",
    "view_dd.observe(view_switcher, names = 'value')\n",
    "view_switcher({'new': 'Document Topics'})\n",
    "\n",
    "display(corpus_dd, view_dd, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Words\n",
    "\n",
    "That's the end. Feel free to explore the models above or to hack the notebook to do your own thing.\n",
    "\n",
    "You might notice some things that could be improved as you go. For example, a common OCR error with older text is reading \"s\" as \"f\". Or you might wish you could do something with the interface but find that you cannot.\n",
    "\n",
    "It bears repeating that this has been a very shallow skim through applying topic modelling to this collection, designed just to give some sort of sense of the possibilities and some pointers to where to get started. Whether or not any of the topics produced here are actually useful or mean anything I cannot say but it is certainly built upon very shaky foundations.\n",
    "\n",
    "To create useful tools or do a serious analysis with methods like these requires a good understanding of how they work, and you certainly will not get that from this notebook. But perhaps it can get you started on the journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This notebook is an output from the [TNA/RLUK Professional Fellowship](https://www.rluk.ac.uk/) \"Collaborative Experimentation: Research Software Prototyping for Co-Learning and Exploration in Cultural Heritage\", which investigated RSE-led prototyping as a method of interdisciplinary collaboration and co-learning in digital cultural heritage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
