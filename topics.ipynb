{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data... there is an API (see https://developers.wellcomecollection.org/docs/examples), but it limits to 10,000 results... in any case, it is straightforward to work with a snapshot... we can use some lightly adapted code from https://developers.wellcomecollection.org/docs/examples/working-with-snapshots-of-the-api to acquire the snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import gzip\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "\n",
    "snapshot_url = \"https://data.wellcomecollection.org/catalogue/v2/works.json.gz\"\n",
    "\n",
    "data_dir = Path(\"./data\").resolve()\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "file_name = Path(snapshot_url).parts[-1]\n",
    "zipped_path = data_dir / file_name\n",
    "unzipped_path = zipped_path.with_suffix(\"\")\n",
    "\n",
    "# check whether the file already exists before doing any work\n",
    "if not unzipped_path.exists():\n",
    "  if not zipped_path.exists():\n",
    "\n",
    "    # make a request to the snapshot URL and stream the response\n",
    "    r = requests.get(snapshot_url, stream=True)\n",
    "    \n",
    "    # use the length of the response to create a progress bar for the download\n",
    "    download_progress_bar = tqdm(\n",
    "      unit=\"bytes\",\n",
    "      total=int(r.headers[\"Content-Length\"]),\n",
    "      desc=f\"Downloading {file_name}\",\n",
    "    )\n",
    "\n",
    "    # write the streamed response to our file in chunks of 1024 bytes\n",
    "    with open(zipped_path, \"wb\") as f:\n",
    "      for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "          f.write(chunk)\n",
    "          download_progress_bar.update(len(chunk))\n",
    "\n",
    "      download_progress_bar.close()\n",
    "\n",
    "  # open the zipped file, and the unzipped file\n",
    "  with gzip.open(zipped_path, \"rb\") as f_in, open(unzipped_path, \"wb\") as f_out:\n",
    "    unzip_progress_bar = tqdm(\n",
    "      unit=\"bytes\",\n",
    "      total=f_in.seek(0, io.SEEK_END), # measure the unzipped length of the zipped file using `.seek()`\n",
    "      desc=f\"unzipping {file_name}\",\n",
    "    )\n",
    "\n",
    "    # we used `.seek()` to move the cursor to the end of the file, so we need to\n",
    "    # move it back to the start before we start reading\n",
    "    f_in.seek(0)\n",
    "\n",
    "    # read the zipped file in chunks of 1MB\n",
    "    for chunk in iter(lambda: f_in.read(1024 * 1024), b\"\"):\n",
    "      f_out.write(chunk)\n",
    "      unzip_progress_bar.update(len(chunk))\n",
    "\n",
    "    unzip_progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll collect a set of interest. In the interests of speed we'll go for something small. Let's find all the books with typhoid as a subject. ... We start by iterating works.json looking for such books. works.json is a JSONL file, so each line is a separate JSON record. This makes line-by-line iteration an efficient way to read it ...but also slow when we read the whole file... so I could either pre-cook a special works.json... or use the web API after all..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESTARTING... LET'S TRY USING THE API\n",
    "Statistical methods like topic modelling probably should really be used with large volumes of data.\n",
    "To keep this example reasonably small (and therefore fast) we'll try to work with a smallish set of books that is large enough to work reasonably well.\n",
    "To begin with, we'll use the catalogue API to search for \"typhoid\".\n",
    "\n",
    "(see https://developers.wellcomecollection.org/docs/examples for much more about working with the API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "catalogue_base_url = 'https://api.wellcomecollection.org/catalogue/v2/'\n",
    "\n",
    "response = requests.get(\n",
    "    catalogue_base_url + 'works',\n",
    "    params={\n",
    "        'include': 'identifiers,subjects',\n",
    "        'pageSize': 100,\n",
    "        'query': 'typhoid',\n",
    "    },\n",
    ")\n",
    "if response.status_code != 200:\n",
    "  print('error', file = sys.stderr)\n",
    "response_data = response.json()\n",
    "for k, v in response_data.items():\n",
    "    if k == 'results': continue #there will be loads of this\n",
    "    print(f'{k}: {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this code, I got 1099 `totalResults`. Your results may differ, depending upon how Wellcome's collection has changed in the meantime. Anyway, this feels like a nice number of texts to start working with. Let's learn some more about them. We'll start by downloading the catalogue data for all of the pages of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#let's have a progress bar\n",
    "catalogue_bar = tqdm(\n",
    "  unit = 'pages',\n",
    "  total = response_data['totalPages'],\n",
    "  desc = 'downloading catalogue data',\n",
    ")\n",
    "\n",
    "#We already got the first page of results in the previous cell\n",
    "catalogue_bar.update(1)\n",
    "works = response_data['results']\n",
    "\n",
    "#Now we'll add all of the other pages of results to the list \"works\"\n",
    "while 'nextPage' in response_data:\n",
    "  response = requests.get(response_data['nextPage'])\n",
    "  catalogue_bar.update(1)\n",
    "  if response.status_code != 200:\n",
    "    print('error', file = sys.stderr)\n",
    "  response_data = response.json()\n",
    "  works.extend(response_data['results'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the catalogue data for our \"typhoid\" works, let's get a sense of what this covers. We'll just look at the contents of the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON as json_display\n",
    "json_display(works[0], expanded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a lot of data! We are interested in text about typhoid, so let's focus on the type of work that this is (is it something written, or something else, like a drawing or a photograph?) and the subject matter. We can use JSONPath to look this up.\n",
    "\n",
    "We'll start with the \"type\" of the work. The last entry in the above JSON is workType. The label and type look relevant. Let's examine the values that these can take across the whole collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... might want to add a cell about filtering out non-Wellcome items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonpath_ng.ext import parse as json_parse\n",
    "from collections import Counter\n",
    "\n",
    "def count(query, data_list):\n",
    "  empty = 0\n",
    "  counter = Counter()\n",
    "  searcher = json_parse(query)\n",
    "  for datum in data_list:\n",
    "    results = searcher.find(datum)\n",
    "    if len(results) == 0:\n",
    "      empty += 1\n",
    "    else:\n",
    "      for result in results: #we should have a list of DatumInContext\n",
    "                             #this function assumes the value will be hashable, so it does not handle all queries\n",
    "                             #for example, it will not work if \"value\" is a dict or a list\n",
    "        counter[result.value] += 1\n",
    "  return empty, counter\n",
    "\n",
    "def dumpCount(query, data_list, min_proportion = 0):\n",
    "  emptyCount, counter = count(query, data_list)\n",
    "  total = len(data_list)\n",
    "  below_min = 0\n",
    "  for k, v in counter.most_common():\n",
    "    proportion = v/total\n",
    "    if proportion >= min_proportion:\n",
    "      print(f'{v:4}/{total} ({100 * v/total:3.0f}%) {k}')\n",
    "    else:\n",
    "      below_min += 1\n",
    "  if below_min > 0:\n",
    "    print(f'{below_min} results hidden as below minimum proportion of {min_proportion * 100:.0f}%')\n",
    "  if emptyCount > 0:\n",
    "    print(f'{emptyCount:4}/{total} ({100 * emptyCount/total:3.0f}%) have no value')\n",
    "\n",
    "print('workType types:')\n",
    "dumpCount('$.workType.type', works)\n",
    "print()\n",
    "print('workType labels:')\n",
    "dumpCount('$.workType.label', works)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a range of types of works in our results. At the time of writing, 3/4 of the works are books and several others are of types that could reasonably have text (e.g. \"Archives and manuscripts\", \"Student dissertations\", \"E-books\", \"Manuscripts\", \"Journals\". However, given that text is provided by an OCR pipeline, it is only printed texts that are likely to have online text available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the catalogue subject of a work is more complicated. Works in Wellcome Collection are classified according to a range of schemes. If we look in the above JSON, we can also see that the structure is fairly complex, involving a mixture of \"Subjects\" and \"Concepts\". Rather than unpick all this, we'll just look at a part of the structure to get a sense of how things are classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "dumpCount('$.subjects[?(@.type==\"Subject\")].label', works, 0.02)\n",
    "\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', works, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight away, we can see that both Subjects and Concepts are not available for about 1/5 of the collection (e.g. 204/1100 ( 19%) have no value).\n",
    "\n",
    "We can also see that there are a lot of possible values here -- so many that I've written the code to hide all results applying to less than 2% of the works on typhoid.\n",
    "\n",
    "We can also see that the phrase \"typhoid fever\" (with varying capitalization) covers 50% of the Subjects and 63% of the Concepts. This suggests that these specific values will get pretty good results in a search. What we cannot tell from this is how many of the works covered by other concepts are actually relevant.\n",
    "\n",
    "[This could be a good place to introduce the difference between Wellcome and non-Wellcome works and to see what effect filtering down to just Wellcome has.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
