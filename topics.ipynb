{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through an engagement with Wellcome Collection's catalogue API for the purpose of working with their digitised text. It works through examples of accessing and working with catalogue metadata, acquiring the raw text of books held by Wellcome Collection and the very basics of topic modelling it. It perhaps records the process of an initial hack at a dataset to get a sense of it --- which of course is a very different thing from a proper understanding.\n",
    "\n",
    "The intent is to help in getting started at working with this text. The topic models generated here are just for demonstration of \"how\" and may not be very interesting in themselves. Nothing in here is necessarily (or even likely to be!) the best way to do a given thing, but it will show you **a** way.\n",
    "\n",
    "While this notebook is an outcome of a collaboration with Wellcome, I am not part of their development team and do not have expert knowledge of how their catalogue works. For the most part, I have worked with it in a hacky sort of way to achieve particular goals and I do not necessarily understand it well. There are surely some mistakes in what I have said and done below and I am sure that there are bugs. Patches are very welcome.\n",
    "\n",
    "See https://developers.wellcomecollection.org/docs/examples for much more about working with the API, including more fully developed examples of specific activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigger Warning**\n",
    "\n",
    "This notebook engages with old books. Opinions expressed and language used in those books may well be insensitive or even offensive from a modern perspective. I am not aware of anything problematic in the examples that I used, but some elements of the notebook are quite dynamic and there is at least a small risk that you will encounter something unpleasant in exploring old text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's begin by installing the dependencies that we need and doing a little bit of setup.\n",
    "\n",
    "The next cell may take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "#You can see helper.py in the folder view on the left.\n",
    "#It contains functions that this notebook uses.\n",
    "#This lets us hide distracting details/complexity, but you can\n",
    "#look inside it if you want to know how these things are working.\n",
    "import helper\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from jsonpath_ng.ext import parse as json_parse\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from yaspin import yaspin\n",
    "import ipywidgets\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.display import JSON as json_display\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "seaborn.set_theme(rc={'figure.figsize':(20,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start this journey by using the catalogue API to search for \"typhoid\".\n",
    "\n",
    "This seems to return a large enough set of results to be interesting for demonstration, but a small enough set of results to be manageable.\n",
    "\n",
    "We wil call the catalogue API to perform this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first page of catalogue results\n",
    "#Parameters that we pass to this function will be passed on to the catalogue API\n",
    "#See https://developers.wellcomecollection.org/api/catalogue more more params\n",
    "def get_cat_page(**kwargs):\n",
    "  params = {\n",
    "    'include': 'identifiers,subjects,production,languages,items', #items is the one that will allow us to find the text\n",
    "    'pageSize': 100, #number of results per page -- 100 is the maximum\n",
    "  }\n",
    "  for k, v in kwargs.items():\n",
    "    params[k.replace('_', '.')] = v\n",
    "  response = helper.get('https://api.wellcomecollection.org/catalogue/v2/works', params)\n",
    "  return response.json()\n",
    "\n",
    "page = get_cat_page(query='typhoid') #query=typhoid will be passed through to the catalogue API\n",
    "for key, value in page.items():\n",
    "  if key != 'results': #the actual results are an overwhelming amount of output so don't show them yet\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have got one page of catalogue entries back from the API. The API returns results in pages of up to 100 results at a time.\n",
    "\n",
    "When I wrote this cell, I got 12 pages of results for a total of 1105 results. You may have different numbers, depending upon how Wellcome's collection has changed in the meantime.\n",
    "\n",
    "The actual entries are in the `results` key, but before we look at them, let's get *all* of the results.\n",
    "\n",
    "You can see above that our page of results includes a pointer to the next page. So we can write a function to get all of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the first page of a set of catalogue results, get a complete list of pages\n",
    "def get_cat_pages(page):\n",
    "  bar = tqdm(\n",
    "    unit = 'pages',\n",
    "    total = page['totalPages'] - 1, #-1 because we already have the first page\n",
    "    desc = 'Downloading catalogue data',\n",
    "  )\n",
    "  pages = [page]\n",
    "  while 'nextPage' in page:\n",
    "    response = helper.get(page['nextPage'])\n",
    "    page = response.json()\n",
    "    pages.append(page)\n",
    "    bar.update(1)\n",
    "  return pages\n",
    "\n",
    "#Given a list of pages, get all of the catalogue entries listed in those pages\n",
    "def cat_entries(pages):\n",
    "  entries = []\n",
    "  for page in pages:\n",
    "    entries.extend(page['results'])\n",
    "  return entries\n",
    "\n",
    "entries = cat_entries(get_cat_pages(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `get_cat_pages` has got all of the pages of catalogue entries back for us.\n",
    "\n",
    "We have then collected all of the entries into one list called `entries`.\n",
    "\n",
    "So let's see what is inside a catalogue entry --- watch out, there is about to be a lot of output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_display(entries[0], expanded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a lot of data, and I believe we can get more if we keep adding to the `items` parameter up in `get_cat_page`.\n",
    "\n",
    "We are interested in **text** about typhoid, so let's focus on the type of work that this entry describes --- is it something written, or something else, like a drawing or a photograph?\n",
    "\n",
    "The entry is in JSON format so we can use JSONPath to look this up.\n",
    "\n",
    "We'll start with the \"type\" of the work. On entry in the above JSON is `workType`. The `label` and `type` look relevant. Let's examine the values that these can take across the whole collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell uses dumpCount from helper.py.\n",
    "#dumpCount takes a JSONPath query and a list of JSON objects\n",
    "#It prints text describing query results.\n",
    "\n",
    "print('workType types:')\n",
    "helper.dumpCount('$.workType.type', entries)\n",
    "print()\n",
    "print('workType labels:')\n",
    "helper.dumpCount('$.workType.label', entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm checking the `type` just to reassure myself about the data model. I'm not an expert in it, so I'm kind of figuring out how it works as I go.\n",
    "\n",
    "As I write this, the only `type` that I see is `Format`, which encourages me to believe that I don't need to worry about this value and can just think about the labels. If you need to be sure that you're using the data model correctly you might need to learn more about this.\n",
    "\n",
    "`label` shows that there are a range of types of works in our results. At the time of writing, 3/4 of the works are books and several others are of types that could reasonably have text, such as \"Archives and manuscripts\", \"Student dissertations\", \"E-books\", \"Manuscripts\", \"Journals\".\n",
    "\n",
    "Given that the text available through this API is provided by an OCR pipeline, it is only printed texts that are likely to have online text available. So let's filter down to just books, e-books and journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filters down to a list of just books, e-books and journals.\n",
    "#We cannot use JSONPath to do this because JSONPath can only check values for the\n",
    "#purpose of filtering lists, and our data appears to JSONPath code as single JSON objects\n",
    "#(re e.g. https://stackoverflow.com/a/43737750)\n",
    "def printed_entries(entries):\n",
    "  return [entry for entry in entries if entry['workType']['label'] in ['Books', 'E-books', 'Journals']]\n",
    "\n",
    "entries = printed_entries(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've filtered down our `entries` list to cover just works that seem likely to be printed.\n",
    "\n",
    "We might also be interested in the catalogued subjects of these works.\n",
    "\n",
    "Working out the catalogue subject of a work is more complicated. Works in Wellcome Collection are classified according to multiple schemes. If we look in the above JSON, we can also see that the structure there is fairly complex, involving a mixture of `subject` and `concept`.\n",
    "\n",
    "But, tather than unpick all of this, we'll just look at a part of the structure to get a sense of how things are classified. We'll just show `subject`, and only for subjects that apply to at least 5% of the works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have two columns of numbers.\n",
    "\n",
    "Before, we were dealing with formats. Each catalogue entry refers to a single physical object --- a book, a journal, a picture, etc etc --- and so it has only one format.\n",
    "\n",
    "Now, we are dealing with subjects and each catalogue entry may have more than one subject, so count both \"out of entries\" and \"out of subjects\".\n",
    "\n",
    "Let's unpack this with a small example. To begin, we can look at the subjects in a sample of ten entries. We will get 10 specific IDs, so that I can write these explanations with some hope that you are seeing what I saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first version of this example used the first 10 works in entries, but the catalogue might change.\n",
    "#So now I get the same works by identifier, but I've changed the last two to make a better example.\n",
    "\n",
    "#Given a list of entries and list of ids, return entries with an id in the list of ids\n",
    "def get_entries_by_ids(entries, ids):\n",
    "  return [entry for entry in entries if entry['id'] in ids]\n",
    "\n",
    "#Return markdown-formatted list of some label within an entry\n",
    "def dump_labels(entries, #A list of entries\n",
    "                jsonpath, #Jsonpath to find a \"labelled thing\" (something with both the properties `label` and `id`)\n",
    "                entity_name, #A friendly name for the labelled thing\n",
    "                emphasis_pattern = ''): #A regular expression: anything matching it will be in bold\n",
    "  searcher = json_parse(jsonpath)\n",
    "  output = []\n",
    "  for entry in entries:\n",
    "    labelled_things = searcher.find(entry)\n",
    "    output.append(f'* {entry[\"title\"]} (id: {entry[\"id\"]}) [**{len(labelled_things)}** {entity_name}]')\n",
    "    for labelled_thing in labelled_things:\n",
    "      label, id = (labelled_thing.value[\"label\"], labelled_thing.value[\"id\"]);\n",
    "      x = f'{label} (id: {id})'\n",
    "      if emphasis_pattern and re.match(emphasis_pattern, label):\n",
    "        output.append(f'  * **{x}**')\n",
    "      else:\n",
    "        output.append(f'  *   {x}')\n",
    "  return Markdown('\\n'.join(output))\n",
    "\n",
    "sample = get_entries_by_ids(entries,\n",
    "                            ['bxa3fqrw','f56ccxnd','jf55amap','pw7sr9zn','q5pqqysq','qzy6ufxp','rxyt9ncw','vqhzjwd5','ab2ncfmj', 'sqwwchy7'])\n",
    "\n",
    "display(dump_labels(sample, #our small sample of entries \n",
    "                    '$.subjects[?(@.type==\"Subject\")]', #jsonpath identifying the entity we are interested in\n",
    "                    'subjects', #a nice name for the entity\n",
    "                    '^Typhoid [Ff]ever$' #any text in the output that matches this pattern will be rendered as bold\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is a total of 10 entries for printed works. These have varying numbers of subjects, totalling 18 (3 * 1 + 6 * 2 + 1 * 3 = 18).\n",
    "\n",
    "I've highlighted all uses of \"Typhoid Fever\" as a subject. You may notice that there are two different ways of identifying the general subject of \"typhoid fever\" -- one spelling fever with a capital F and one with a lower case f. These two spellings also have distinct IDs. If we wanted to find all books with this general subject then we would have to use both spellings. Even then, we would have to watch out for cases like that copy of \"On typhoid fever\", which has both spellings.\n",
    "\n",
    "You may also notice that there are two copies of William Thomson's \"On typhoid fever\". As it happens, one of these copies has two different \"Typhoid fever\" subjects, and one has only one of them.\n",
    "\n",
    "Let's now run `dumpCount` over the same ten entries, first to get the titles, then to get the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.title', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"On typhoid fever\" appears twice and all of the others appear once. \"On typhoid fever\" is therefore 20% of the sample.\n",
    "\n",
    "The numbers here are out of 10, because there are 10 entries.\n",
    "\n",
    "We only get one list of numbers because the number of titles equals the number of entries, so a second list would just be exactly the same as the first. `dumpCount` is written not to give us two lists when this happens.\n",
    "\n",
    "Now let's look at subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different numbers of entries (10) and subjects (18), as we saw above. Because of this we get two lists: the \"entries\" list calculates percentages as a proportion of the number of works and the \"hits\" list calculates percentages as a proportion of the number of subjects.\n",
    "\n",
    "The left-hand \"entries\" column is still counting \"by entry\" --- each number is out of 10, the number of entries.\n",
    "\n",
    "4/10 entries have the subject \"Typhoid Fever\", and another 3/10 have the subject \"Typhoid fever\" (upper-case vs lower-case \"f\").\n",
    "\n",
    "Looking a few cells above, we can see that the first entry (\"Typhoid fever and chronic typhoid carriers\") has the subjects \"Typhoid Fever - epidemiology\" and \"Typhoid Fever - transmission\", so it effectively appears twice in the left-hand column, once for each subject. All entries will be counted once in this column for each subject that they have. Because of this, the total of entries in the left-hand column is greater than 10 --- in fact it will be 18, the total number of subjects. If we count up the percentages in this column, they will come to 180% (1 * 40 + 1 * 30 + 11 * 10 = 40 + 30 + 110 = 180).\n",
    "\n",
    "The right-hand \"hits\" column is counting \"by subject\" --- each number is out of 18, the total number of subjects possessed by all of the books. Just as \"On typhoid fever\" appeared twice in our lists of titles, some subjects appear more than once when we list all of the subjects of all of the books. If we count up the percentages in this column, they will come to 100% (actually, they come out slightly higher, but this is because of rounding errors).\n",
    "\n",
    "Note also that the inconsistent nature of the data leads to some misrepresentation. If we normalize by case, the proportions will change a little. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of entries, returns a new list of entries with normalized subject labels\n",
    "#this is rough and ready -- it normalizes label case but ignores other attributes, such as id\n",
    "#Unlike most other functions in this code, this one returns clones of the original, not pointers to the original\n",
    "def normalize_subjects(original_entries):\n",
    "  entries = [deepcopy(entry) for entry in original_entries]\n",
    "  for entry in entries:\n",
    "    subjects = entry['subjects']\n",
    "    seen = set()\n",
    "    normalized_subjects = []\n",
    "    for subject in subjects:\n",
    "      if subject['type'] != 'Subject': continue #if it is not actually a subject, move on to the next subject\n",
    "      lowered = subject['label'].lower()\n",
    "      if lowered in seen: continue #if this work already has a subject with this label, move on to the next subject\n",
    "      subject['label'] = lowered\n",
    "      seen.add(lowered)\n",
    "      normalized_subjects.append(subject)\n",
    "    entry['subjects'] = normalized_subjects\n",
    "  return entries\n",
    "\n",
    "normalized_sample = normalize_subjects(sample) #normalized_sample is a misnomer, we are only normalizing subject label\n",
    "display(dump_labels(normalized_sample, '$.subjects[?(@.type==\"Subject\")]', 'subjects', '^typhoid fever$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general \"Typhoid Fever\" subject is now consistently \"typhoid fever\".\n",
    "\n",
    "We still have our same ten titles but now only 16 subjects because \"Typhoid fever: a history\" and the first copy of \"On typhoid fever\" no longer have the same subject label listed twice with different cases.\n",
    "\n",
    "We can perform the same analysis with this slightly cleaner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', normalized_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see easily see that 60% of the works have the most general \"typhoid fever\" subject, which is also 38% of all of the subjects covered.\n",
    "\n",
    "Which might be roughly what we would expect in a corpus based on a search for \"typhoid\".\n",
    "\n",
    "There is more we could do to clean this data, and to make sure of our assumptions about the data model. For example, I am assuming that one work cannot have two completely identical subjects. But let's move on.\n",
    "\n",
    "You might recall that there is more than one way of talking about classifications in the data model, so let's take a quick look at that back in our full set of entries describing printed works. We'll again look at \"subjects\" applying to at least 5% of books in our `entries`, but now we'll look at \"concepts\" too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between \"Typhoid Fever\" and \"Typhoid fever\" is back with us because we only lower-cased subject labels in a sample that we copied out of `entries`. \n",
    "\n",
    "Still, this gives us an impression of the state of our corpus. At time of writing, 18% of these works have no subjects and 19% have no concepts.\n",
    "\n",
    "We can also see that the subjects and concepts are quite similar. The concepts look maybe finer-grained, but I'm mostly guessing.\n",
    "\n",
    "If you want to see more of the subjects/concepts in the collection, make the number at the end of each `dumpCount` call smaller, or remove it to see all of them. There will be a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty may be that Wellcome's catalogue includes texts belonging to other collections. These could be classified in different ways.\n",
    "\n",
    "So let's assume that we are interested in searching works actually held by Wellcome itself and limit down to them.\n",
    "\n",
    "The way that was suggested to me to do this was to look for works held on either open shelves or in closed stores. This seems to make sense --- perhaps it needs a tweak for purely digital works such as E-books, at the time I was dealing with text from the 1700s so I wasn't too concerned about electronic formats. For purposes of this notebook we will continue not to worry about that, so let's filter down our `entries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All availability ids within entries:')\n",
    "helper.dumpCount('$.availabilities[*].id', entries)\n",
    "print()\n",
    "\n",
    "def wellcome_entries(entries):\n",
    "  open_searcher   = json_parse(\"$.availabilities[?(@.id=='open-shelves')].id\")\n",
    "  closed_searcher = json_parse(\"$.availabilities[?(@.id=='closed-stores')].id\")\n",
    "  return [entry for entry in entries if len(open_searcher.find(entry)) > 0 or len(closed_searcher.find(entry)) > 0]\n",
    "\n",
    "total_entries = len(entries)\n",
    "entries = wellcome_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} printed works are available in closed and/or open stores (therefore held by Wellcome itself)')\n",
    "total_entries = len(entries)\n",
    "print('These break down as:')\n",
    "helper.dumpCount('$.workType.label', entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `entries` uses the availabilities `online`, `closed-stores`, and `open-shelves`.\n",
    "\n",
    "677 (at time of writing) `entries` are held by Wellcome itself, so we have replaced our old `entries` list with just these ones. Most of these entries describe books.\n",
    "\n",
    "Now that we have done this, we can look again at concepts and subjects, to see what the coverage is like for the particular works that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', entries, 0.05)\n",
    "\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', entries, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time of writing, the subjects `Typhoid Fever - epidemiology` and `Typhoid Fever` are both well represented among printed works held at Wellcome. 24% of works have the subject `Typhoid Fever` and 22% have the subject `Typhoid Fever - epidemiology`. Up to 46% of the whole corpus thefore has one or other of these subjects --- but some works might have both subjects, so the real proportion might be lower.\n",
    "\n",
    "A very large 63% of works have the concept `Typhoid Fever` at time of writing --- a proportion that is even more significant given that 24% of works have no concept at all.\n",
    "\n",
    "As there are works missing concepts, lack of classification information is not explained only by a work not being held at Wellcome (or we have done something wrong).\n",
    "\n",
    "We could keep digging, but let's just define a couple of functions for getting at subjects and concepts and then move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects(entry):\n",
    "  #sorted just because I like repeatability\n",
    "  return sorted(set([subject.lower() for subject in helper.list_by_jsonpath('$.subjects[?(@.type==\"Subject\")].label', [entry])]))\n",
    "\n",
    "def concepts(entry):\n",
    "  #sorted just because I like repeatability\n",
    "  return sorted(set([concept.lower() for concept in helper.list_by_jsonpath('$.subjects..*[?(@.type==\"Concept\")].label', [entry])]))\n",
    "\n",
    "print('Quick check -- subjects of first entry:')\n",
    "print('\\n'.join(subjects(entries[0])))\n",
    "\n",
    "print()\n",
    "print('Quick check -- concepts of first entry:')\n",
    "print('\\n'.join(concepts(entries[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to looking at text, let's also get a sense of when works were published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dates of printed Wellcome works, by frequency (min 1%)\")\n",
    "helper.dumpCount('$.production[*].dates[*].label', entries, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this lists only a few dates, it is enough for us to see that the date format is not completely consistent (at least at time of writing). Sometimes we get a year, sometimes we get a year inside square brackets.\n",
    "\n",
    "Let's see how many of the dates do not consist entirely of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = helper.list_by_jsonpath('$.production[*].dates[*].label', entries)\n",
    "not_a_number = Counter([x for x in dates if not x.isnumeric()])\n",
    "print(sorted(not_a_number.keys()))\n",
    "print()\n",
    "print(f'Total \"bad\" dates: {not_a_number.total()}/{len(dates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are quite a few \"square brackets\" cases, but also date ranges, copyright symbols, question marks and occasionally snippets of text.\n",
    "\n",
    "At time of writing, 40% of dates are not numbers, which seems like a lot.\n",
    "\n",
    "Let's try just stripping out square brackets: this discards some information that might be important, but for now we just want a rough sense of the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [x.strip('[]') for x in dates]\n",
    "not_a_number = Counter([x for x in dates if not x.isnumeric()])\n",
    "print(sorted(not_a_number.keys()))\n",
    "print()\n",
    "print(f'Total \"bad\" dates: {len(not_a_number)}/{len(dates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time of writing, this reduces the \"bad date\" proportion to 21%.\n",
    "\n",
    "We could do more, but let's carry on with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dates of printed Wellcome works, roughly chronologically ordered, discarding non-numbers:\")\n",
    "dates = sorted([int(x) for x in dates if x.isnumeric()])\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a sense of the corpus, we can see that the works span from 1762 to the modern day.\n",
    "\n",
    "18 and 19 are questionable numbers. These could refer to very early works but my first guess is that these are the first two digits of a century.\n",
    "\n",
    "Let's just drop those two numbers and then look at this data in a more visual form. We'll start by writing a function that pulls together the above rough date-handling techniques, then use that to get our dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work out the date for an entry, doing some very rough cleaning\n",
    "def date(entry):\n",
    "  dates = helper.list_by_jsonpath('$.production[*].dates[*].label', [entry])\n",
    "  if len(dates) == 0: return None\n",
    "  date = dates[0] #we won't worry about dealing with more than one date\n",
    "  date = date.strip('[]')\n",
    "  if not date.isnumeric(): return None\n",
    "  date = int(date)\n",
    "  if date > 1000 and date < 2100: return date\n",
    "  return None\n",
    "\n",
    "#recompute the dates\n",
    "dates = [date(entry) for entry in entries] #get date from all entries\n",
    "dates = sorted([date for date in dates if date]) #sort, filtering out the Nones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate some charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_dates = Counter(dates)\n",
    "total = 0\n",
    "cumulative = {}\n",
    "for year in set(dates):\n",
    "  total += counted_dates[year]\n",
    "  cumulative[year] = total\n",
    "\n",
    "xlim = (helper.down(dates[0], 50), helper.up(dates[-1], 50))\n",
    "xticks = range(xlim[0], xlim[1] + 1, 25)\n",
    "\n",
    "ylim = (0, helper.up(counted_dates.most_common(1)[0][1], 2))\n",
    "ax = seaborn.scatterplot(counted_dates)\n",
    "ax.set(\n",
    "  title = 'Publications per year',\n",
    "  xlabel = 'Year',\n",
    "  ylabel = 'Works',\n",
    "  ylim = ylim,\n",
    "  yticks = range(ylim[0], ylim[1] + 1, 2),\n",
    "  xlim = xlim,\n",
    "  xticks = xticks,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "ylim = (0, helper.up(total, 50))\n",
    "ax = seaborn.lineplot(cumulative)\n",
    "ax.set(\n",
    "  title = 'Cumulative publications per year',\n",
    "  xlabel = 'Year',\n",
    "  ylabel = 'Cumulative works',\n",
    "  ylim = ylim,\n",
    "  yticks = range(ylim[0], ylim[1] + 1, 25),\n",
    "  xlim = xlim,\n",
    "  xticks = xticks,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I write, these charts show that Wellcome's own collection of printed works that are returned for a search on 'typhoid' were published mainly between the late 1800s and early 1900s -- discounting all the ones with dates that were not easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's get on to actually accessing the text of our works. We'll keep the ones with funny dates, but quickly throw away the ones that are not in English, so that we're working with a single language.\n",
    "\n",
    "We'll do this by finding all ids belonging to books that have a non-English language, and then throwing away those ids. We do it like this because some books may have more than one language listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return all entries that do not have any non-English languages listed\n",
    "def english_entries(entries):\n",
    "  non_english_ids = set(helper.list_by_jsonpath('$.languages[?(id!=\"eng\")].`parent`.`parent`.id', entries))\n",
    "  return [entry for entry in entries if not entry['id'] in non_english_ids]\n",
    "\n",
    "entries = english_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} printed works held by Wellcome are in English (or at least are not labelled as non-English)')\n",
    "total_entries = len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have explored what the catalogue can tell us a little and learned a little bit about the texts in the corpus.\n",
    "\n",
    "There is certainly more you could do to get a coherent corpus, for example looking at a particular place and/or time. But let's work with the corpus that we now have: results from a search for \"typhoid\", from any time, but only in English.\n",
    "\n",
    "The next step is to find out which ones actually have digitised text available. The text is made available through a IIIF manifest, so we go looking for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: If there are multiple URLs, do I get multiple entries here?\n",
    "#      That would mean more than one manifest for a work, which hopefully doesn't happen.\n",
    "def ocr_entries(entries):\n",
    "  #This bit of JSONPath finds every member of the `items` array that ultimately turns out to contain a IIIF location.\n",
    "  #It then gets the grandparent of that iten, which should be the data structure that we started with.\n",
    "  #In this way, we can filter our list of printed Wellcome works to just those for which we have OCR'd text.\n",
    "  return helper.list_by_jsonpath('$.items[?(@.locations[*].locationType.id=\"iiif-presentation\")].`parent`.`parent`', entries)\n",
    "\n",
    "entries = ocr_entries(entries)\n",
    "print(f'{len(entries)}/{total_entries} entries have a IIIF manifest.')\n",
    "total_entries = len(entries)\n",
    "\n",
    "#Return a simple text description from an entry\n",
    "#Very much assuming single-entry lists here\n",
    "def description(entry):\n",
    "  return f'id {entry[\"id\"]} -- {entry[\"title\"]}, published {entry[\"production\"][0][\"dates\"][0][\"label\"]}'\n",
    "\n",
    "print('The first of these in the list is ' + description(entries[0]))\n",
    "\n",
    "#Get the URL of the IIIF manifest describing an entry\n",
    "def iiif_manifest_url(entry):\n",
    "  return helper.expect_one(helper.list_by_jsonpath('$.items[*].locations[?(@.locationType.id=\"iiif-presentation\")].url', [entry]))\n",
    "\n",
    "print('Its IIIF manifest is at', iiif_manifest_url(entries[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to click on that manifest and take a look. Next we will use the information in it to extract the OCR'd text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iiif_manifest(url):\n",
    "  return helper.get(url).json()\n",
    "\n",
    "def text_url(manifest):\n",
    "  return helper.expect_one(helper.list_by_jsonpath('$.sequences[*].rendering[?(@.format=\"text/plain\")].@id', [manifest]))\n",
    "\n",
    "print('Plain text at', text_url(get_iiif_manifest(iiif_manifest_url(entries[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there should only be one URL, and you can click on it if you would like to take a look. As a large body of unformatted text, it is not very easy to read.\n",
    "\n",
    "Now lets get that text so that we can do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_text(url):    \n",
    "  response = helper.get(url)\n",
    "  return response.text\n",
    "\n",
    "text = get_plain_text(text_url(get_iiif_manifest(iiif_manifest_url(entries[0]))))\n",
    "sentences = text.split('.') #an approximation to a list of sentences from the book\n",
    "print(\"Some randomly-selected sentences, just to prove that we have got some text from a work:\")\n",
    "print()\n",
    "print()\n",
    "print('.\\n\\n'.join(random.choices(sentences, k=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate some other analyses. To make sure that we're looking at the same thing, I'm going to switch to getting the text that was the first work in the list when I ran this notebook --- it may or may not be the first work in the list for you.\n",
    "\n",
    "We'll start by getting that text, in the same way that we did above, but all in one cell this time. We'll also take the opportunity to define some functions that we can reuse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the example that I am using\n",
    "sample = helper.expect_one([entry for entry in entries if entry['id']=='kbspden9'])\n",
    "print(f'The example work is {description(sample)}')\n",
    "\n",
    "def get_plain_text_from_entry(entry):\n",
    "  return get_plain_text(text_url(get_iiif_manifest(iiif_manifest_url(entry))))\n",
    "\n",
    "text = get_plain_text_from_entry(sample)\n",
    "\n",
    "sentences = text.split('.') #an approximation to a list of sentences from the book\n",
    "print(\"Some randomly-selected sentences, just to prove that we have got some text from a work:\")\n",
    "print()\n",
    "print()\n",
    "print('.\\n\\n'.join(random.choices(sentences, k=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall generate a word cloud from that work's text. The word cloud will ignore common words that do not convey much informaion (stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you would like to generate a cloud with different settings, adjust some of these parameters\n",
    "wc = WordCloud(width=1024, height=800, min_font_size=20, \n",
    "               margin=75, #space between words\n",
    "               background_color='white', color_func=lambda *args, **kwargs: 'black', #white background, black text\n",
    "               max_words=10,\n",
    "#               stopwords=set() #uncomment this line to prevent stopword removal, or populate the set to give your own stopwords. The default stopwords are here: https://github.com/amueller/word_cloud/blob/main/wordcloud/stopwords.\n",
    ")\n",
    "freq = wc.process_text(text)\n",
    "wc.generate_from_frequencies(freq) #we can just to wc.generate(example_text), but this way we get the frequency table, too\n",
    "display(pd.DataFrame(Counter(freq).most_common()[0:wc.max_words], columns=['word','count']).set_index('word'))\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows the frequencies of the top words.\n",
    "\n",
    "The cloud shows the same thing. It indicates that \"fever\" is the most frequently occuring word in the text, with \"case\", \"one\" and \"â\" also coming up a lot.\n",
    "\n",
    "It might be interesting to compare the table of frequencies with the cloud --- word clouds are nice and intuitive, but do these sizes really reflect the proportions in the table (and does it matter if they are not quite right)? Just something to think about.\n",
    "\n",
    "Anyway, that \"â\" is curious. It might well be a transcription error, let's look for it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_circumflex_mentions = [x for x in sentences if 'â' in x.lower()]\n",
    "print(f'{len(a_circumflex_mentions)}/{len(sentences)} sentences contain \"â\"')\n",
    "print('\\nHere are the first 10 of them:\\n')\n",
    "print('.\\n\\n'.join(a_circumflex_mentions[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of these seem are likely to be apostrophes or quote marks, as in \"Budd's original essay\" or \"the 'Lancet,'\". Others are harder to interpret. Using the URLs generated by the next cell, we can look up the work, and searchable images of its pages, in Wellcome's catalogue.\n",
    "\n",
    "There are likely better ways to do this, but I have not delved into those parts of the API --- I was mainly interested in getting the whole text of a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Catalogue:      https://wellcomecollection.org/search/works?query=' + sample['id'])\n",
    "print('Digitised work: https://wellcomecollection.org/works/' + sample['id'] + '/items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the latter URL to search for some of the above sentences, I see that 'â' seems to show up as either an apostrophe/quote mark or an em-dash. So I'm going to assume it is getting rendered in place of some kinds of punctuation. Let's see if we can clean this out of our text, and what that does to the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('â', '')\n",
    "freq = wc.process_text(text)\n",
    "wc.generate_from_frequencies(freq) #we can just to wc.generate(example_text), but this way we get the frequency table, too\n",
    "display(pd.DataFrame(Counter(freq).most_common()[0:10], columns=['word','count']).set_index('word'))\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now \"may\" is showing up as one of our top words. I'd argue that it may also be quite meaningless, and I could go and look in a similar manner and, if appropriate, remove it too.\n",
    "\n",
    "But let's move on for the moment. Let's assume that the text is now \"clean enough\" with our one little \"a circumflex\" clean up and try something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following https://neptune.ai/blog/pyldavis-topic-modeling-exploration-tool\n",
    "\n",
    "#Rebuild the text into the form that gensim expects\n",
    "#Don't worry too much about this bit, there is a more sensible example of setting up gensim topic models below\n",
    "text = []\n",
    "for word, count in freq.items():\n",
    "  text.extend([word] * count)\n",
    "id2word = Dictionary([text])\n",
    "corpus = [id2word.doc2bow(text)]\n",
    "#pprint(id2word)\n",
    "\n",
    "lda_model = LdaModel(corpus = corpus, id2word = id2word,\n",
    "                     num_topics = 10,\n",
    "                     random_state = 0,\n",
    "                     chunksize = 100,\n",
    "                     alpha = 'auto',\n",
    "                     per_word_topics = True,\n",
    ")\n",
    "#doc_lda = lda_model[corpus]\n",
    "#pprint(lda_model.print_topics())\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's collect all of the text for our corpus.\n",
    "\n",
    "While we are at, we will make some simpler data structures so that we can easily get at the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_works(entries):\n",
    "  bar = tqdm(\n",
    "    unit = 'works',\n",
    "    total = len(entries),\n",
    "    desc = 'Downloading text of works',\n",
    "  )\n",
    "\n",
    "  works = []\n",
    "  for entry in entries:\n",
    "    text = ''\n",
    "    try:\n",
    "      text = get_plain_text_from_entry(entry)\n",
    "    except Exception as e:\n",
    "      print(f'Unable to get text for {description(entry)}')\n",
    "      #print(f'Unable to get text for {description(entry)} [cause: {e}]')\n",
    "      bar.update(1)\n",
    "      continue #do not create an entry if there is no text to work with\n",
    "\n",
    "    works.append({\n",
    "      'id': entry['id'],\n",
    "      'title': entry['title'],\n",
    "      'date': date(entry),\n",
    "      'subjects': subjects(entry),\n",
    "      'concepts': concepts(entry),\n",
    "      'text': text,\n",
    "    })\n",
    "\n",
    "    bar.update(1)\n",
    "  return works\n",
    "\n",
    "works = get_works(entries)\n",
    "print(f'\\nGot text for {len(works)}/{len(entries)} works.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably got text for most but not quite all of the entries.\n",
    "\n",
    "Now we need to clean/preprocess the text. We'll do a few things:\n",
    "\n",
    "1. Remove stopwords (low-information words like \"and\", \"but\")\n",
    "2. Stem: Chop off certain suffixes, so that different forms of the same word are treated as the same word.\n",
    "3. Tokenize: split the text into individual tokens. In this case, into words. #TODO: Check that my tokens are words.\n",
    "4. Lowercase: Put all words into lower-case so that capitalized and uncapitalized words are treated as the same word.\n",
    "5. Drop short tokens: Short tokens are dropped. This will, for example, get rid of single-letter words. Hopefully this gets rid of quite a lot of OCR error.\n",
    "\n",
    "Stemming is a bit of a blunt tool -- just chopping off certain suffixes can lead to treating different words as being the same. \n",
    "Lemmatizing is a more sophisticated alternative to stemming but it requires more compute so we will not try it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(works):\n",
    "  bar = tqdm(\n",
    "    unit = 'works',\n",
    "    total = len(works),\n",
    "    desc = 'Preparing text of works',\n",
    "  )\n",
    "\n",
    "  token_count = 0\n",
    "  tokenized_works = []\n",
    "  for work in works:\n",
    "    destopped = gensim.parsing.remove_stopwords(work['text'])\n",
    "    stemmed = gensim.parsing.stem_text(destopped)\n",
    "    redestopped = gensim.parsing.remove_stopwords(stemmed) #stemming might have resulted in stopwords reappearing\n",
    "    all_tokens = list(gensim.utils.tokenize(redestopped, True)) #True imposes lower case\n",
    "    tokens = [token for token in all_tokens if len(token) >= 3] #Drop very short tokens\n",
    "    if(len(tokens)): #drop documents that are empty after preprocessing\n",
    "      work['tokens'] = tokens\n",
    "      tokenized_works.append(work)\n",
    "    token_count += len(tokens)\n",
    "    bar.update(1)\n",
    "\n",
    "  print(f'Generated {token_count:,} tokens')\n",
    "  return tokenized_works\n",
    "\n",
    "total_works = len(works)\n",
    "works = tokenize(works)\n",
    "print(f'{len(works)}/{total_works} works preprocessed for model')\n",
    "total_works = len(works)\n",
    "\n",
    "#Defined as a function just to avoid polluting the global namespace any further with short-term variables\n",
    "def report_tokens(work):\n",
    "  early_tokens = work['tokens'][0:100]\n",
    "  print()\n",
    "  print(f'First {len(early_tokens)} tokens:')\n",
    "  print(' '.join(early_tokens))\n",
    "\n",
    "  unprocessed_early_tokens = list(gensim.utils.tokenize(work['text']))[0:150]\n",
    "  print()\n",
    "  print(f'For comparison, the first {len(unprocessed_early_tokens)} tokens without preprocessing are:')\n",
    "  print(' '.join(unprocessed_early_tokens))\n",
    "\n",
    "report_tokens(works[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate a topic model. The next cell may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(works, num_topics = 10):\n",
    "  id2word = Dictionary([work['tokens'] for work in works])\n",
    "  corpus = [id2word.doc2bow(work['tokens']) for work in works]\n",
    "  model = LdaModel(corpus = corpus, id2word = id2word,\n",
    "                   num_topics = num_topics, #Number of topics to generate\n",
    "                   random_state = 0,        #Random seed. Keeping this number the same should generate the same topics each time. Changing it should change them.\n",
    "                   chunksize = len(corpus), #Number of documents per training chunk. We are dealing with a small number of docs, so may as well use all of them\n",
    "                   alpha = 'auto',          #Hyperparameter -- we won't get into these\n",
    "                   eta = 'auto',            #Another hyperparameter\n",
    "                   per_word_topics = True,  #Generates some additional information\n",
    "  )\n",
    "  return id2word, corpus, model\n",
    "\n",
    "id2word = None\n",
    "corpus = None\n",
    "lda_model = None\n",
    "\n",
    "with yaspin(text=\"Generating topic model. May take a few minutes.\", timer = True) as sp:\n",
    "  global id2word\n",
    "  global corpus\n",
    "  global lda_model\n",
    "  id2word, corpus, lda_model = model(works)\n",
    "  print(f'\\nTook {int(sp.elapsed_time)}s to generate model.')\n",
    "\n",
    "\n",
    "with yaspin(text='Displaying topic model'):\n",
    "  display(pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the topic distribution per document which --- assuming that I both understand and have implemented this correctly --- should give a sense of which are the main topics for a particular document.\n",
    "\n",
    "(Even if I have implemented it wrong, it still illustrates the possibility.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_topics_bar():\n",
    "  output2 = ipywidgets.Output()\n",
    "\n",
    "  def doc_topics_bar(output, doc_index):\n",
    "    output.clear_output()\n",
    "    with(output):\n",
    "      doc_topics = {'Topic #': [], 'Probability': []}\n",
    "      for topic, probability in lda_model.get_document_topics(corpus[doc_index]):\n",
    "        doc_topics['Topic #'].append(topic)\n",
    "        doc_topics['Probability'].append(probability)\n",
    "      ax = seaborn.barplot(doc_topics, x='Topic #',y='Probability')\n",
    "      ax.set(\n",
    "        title = f'Main topics of \"{works[doc_index][\"title\"]}\" ({works[doc_index][\"id\"]})',\n",
    "        ylim = (0, 1),\n",
    "      )\n",
    "      plt.show()\n",
    "\n",
    "  dd = ipywidgets.Dropdown(\n",
    "    options = [(f'{work[\"title\"]} ({work[\"date\"]}) [{work[\"id\"]}]', index) for index, work in enumerate(works)],\n",
    "    value = 0,\n",
    "    description = 'Work',\n",
    "    layout={'width': '90%'},\n",
    "  )\n",
    "\n",
    "  def switcher(change):\n",
    "    doc_topics_bar(output2, change['new'])\n",
    "\n",
    "  switcher({'new': 0})\n",
    "  dd.observe(switcher, names = 'value')\n",
    "  display(dd, output2)\n",
    "\n",
    "document_topics_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Would really like to sort out the scoping here. Setting things to global generally doesn't end well.\n",
    "#TODO: Would also be good to get the bits of the UI to interact nicely.\n",
    "\n",
    "#Define some corpora to fetch\n",
    "#Note that the Wellcome API can do some filtering for us\n",
    "#There are a couple of examples of that here\n",
    "#See https://developers.wellcomecollection.org/api/catalogue#tag/Works/operation/getWorks for more\n",
    "#Here, params is used to specify params initially passed to the Wellcome API\n",
    "#max_topics is a guess as to the highest number of topics that we can cope with generating for a corpus of this size\n",
    "#max_topics was set by trial and error, your mileage may vary\n",
    "#max_topics limiting factor seems to be what PyLDAVis can cope with\n",
    "CORPORA = {\n",
    "  'Typhoid': {\n",
    "    'params': {'query': 'typhoid'}, #Should result in the same corpus that we built above\n",
    "    'max_topics': 100,\n",
    "  },\n",
    "  'Fever': {\n",
    "    'params': {'query': 'fever'},\n",
    "    'max_topics': 30,\n",
    "  },\n",
    "  '1780-1789': {\n",
    "    'params': {'production.dates.from': '1780-01-01', 'production.dates.to': '1789-12-31'}, #Can do date filter direct in API\n",
    "    'max_topics': 15,\n",
    "  },\n",
    "  '1880-1882': {\n",
    "    'params': {'production.dates.from': '1880-01-01', 'production.dates.to': '1882-12-31'}, #Another date example -- narrower window as generally expect to find more works for more recent dates\n",
    "    'max_topics': 10,\n",
    "  },\n",
    "  'Melancholy': {\n",
    "    'params': {'subjects.label': 'Melancholy'}, #Can do subject filter direct in API\n",
    "    'max_topics': 100,\n",
    "  },\n",
    "}\n",
    "\n",
    "corpus_dd = ipywidgets.Dropdown(\n",
    "  options = list(CORPORA.keys()),\n",
    "  description = 'Corpus',\n",
    ")\n",
    "\n",
    "view_dd = ipywidgets.Dropdown(\n",
    "  options = ['PyLDAVis', 'Document Topics'],\n",
    "  value = 'Document Topics',\n",
    "  description = 'View',\n",
    ")\n",
    "output = ipywidgets.Output()\n",
    "\n",
    "def remodel(works, topic_count):\n",
    "  global id2word\n",
    "  global corpus\n",
    "  global lda_model\n",
    "  output.clear_output()\n",
    "  with yaspin(text=f\"Generating topic model with {topic_count} topics. May take a few minutes.\", timer = True) as sp:\n",
    "    id2word, corpus, lda_model = model(works, topic_count)\n",
    "    print(f'\\nTook {int(sp.elapsed_time)}s to generate model.')\n",
    "  output.clear_output()\n",
    "  if view_dd.value == 'Document Topics':\n",
    "    with output:\n",
    "      document_topics_bar()\n",
    "  else:\n",
    "    view_dd.value = 'Document Topics'\n",
    "\n",
    "def reload(corpus, topic_count):\n",
    "  global works\n",
    "  def get_entries(**kwargs):\n",
    "    return cat_entries(get_cat_pages(get_cat_page(**kwargs)))\n",
    "  def common_filters(entries):\n",
    "    return ocr_entries(english_entries(wellcome_entries(printed_entries(entries))))\n",
    "  def tokens(entries):\n",
    "    return tokenize(get_works(entries))\n",
    "\n",
    "  with output:\n",
    "    if corpus in CORPORA.keys():\n",
    "      try:\n",
    "        works = tokens(common_filters(get_entries(**CORPORA[corpus]['params'])))\n",
    "      except:\n",
    "        return\n",
    "    else:\n",
    "      return\n",
    "\n",
    "  with open(f'{corpus}.p', 'wb') as f:\n",
    "    pickle.dump(works, f)\n",
    "\n",
    "  remodel(works, topic_count)\n",
    "\n",
    "def load(corpus, topic_count):\n",
    "  global works\n",
    "  with open(f'{corpus}.p', 'rb') as f:\n",
    "    works = pickle.load(f)\n",
    "  remodel(works, topic_count)\n",
    "\n",
    "def corpus_switcher(change):\n",
    "  corpus = change['new']\n",
    "\n",
    "  topic_count_slider = ipywidgets.IntSlider(\n",
    "    value = 10,\n",
    "    min = 2,\n",
    "    max = CORPORA[corpus]['max_topics'],\n",
    "    step = 1,\n",
    "    description = f'# Topics (2-{CORPORA[corpus][\"max_topics\"]})',\n",
    "    tooltip = f'Set number of topics (2-{CORPORA[corpus][\"max_topics\"]})',\n",
    "    continuous_update = False,\n",
    "    orientation = 'horizontal',\n",
    "    readout = True,\n",
    "    readout_format = 'd',\n",
    "    layout = {'width': '75%'}\n",
    "  )\n",
    "\n",
    "  def handle_click(button):\n",
    "    output.clear_output()\n",
    "    if button.description[0] == 'L':\n",
    "      load(corpus, topic_count_slider.value)\n",
    "    elif button.description[0] == 'R':\n",
    "      reload(corpus, topic_count_slider.value)\n",
    "\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    display(topic_count_slider)\n",
    "    if(os.path.isfile(f'{corpus}.p')):\n",
    "      loadButton   = ipywidgets.Button(description = 'Load from cache (faster)', layout = {'width': '30%'})\n",
    "      reloadButton = ipywidgets.Button(description = 'Reload from Wellcome (slower)', layout = {'width': '30%'})\n",
    "      loadButton.on_click(handle_click)\n",
    "      reloadButton.on_click(handle_click)\n",
    "      print('Cache detected: using this will save on downloading and preprocessing.')\n",
    "      print('You still have to wait for the modelling.')\n",
    "      display(ipywidgets.HBox([loadButton, reloadButton]))\n",
    "    else:\n",
    "      reload(corpus)\n",
    "\n",
    "def pyldavis():\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    with yaspin(text='Displaying topic model'):\n",
    "      display(pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word))\n",
    "\n",
    "def view_switcher(change):\n",
    "  value = change['new']\n",
    "  output.clear_output()\n",
    "  with output:\n",
    "    if value == 'PyLDAVis': pyldavis()\n",
    "    elif value == 'Document Topics': document_topics_bar()\n",
    "\n",
    "corpus_dd.observe(corpus_switcher, names = 'value')\n",
    "view_dd.observe(view_switcher, names = 'value')\n",
    "view_switcher({'new': 'Document Topics'})\n",
    "\n",
    "display(corpus_dd, view_dd, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
