{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell just installs dependencies -- setting up things that we need\n",
    "\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "#You can see helper.py in the folder view on the left.\n",
    "#It contains functions that this notebook uses.\n",
    "#This lets us hide distracting details/complexity, but you can\n",
    "#look inside it if you want to know how these things are working.\n",
    "import helper\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from jsonpath_ng.ext import parse as json_parse\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import math\n",
    "\n",
    "from IPython.display import JSON as json_display\n",
    "from IPython.core.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESTARTING... LET'S TRY USING THE API\n",
    "Statistical methods like topic modelling probably should really be used with large volumes of data.\n",
    "To keep this example reasonably small (and therefore fast) we'll try to work with a smallish set of books that is large enough to work reasonably well.\n",
    "To begin with, we'll use the catalogue API to search for \"typhoid\".\n",
    "\n",
    "(see https://developers.wellcomecollection.org/docs/examples for much more about working with the API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_base_url = 'https://api.wellcomecollection.org/catalogue/v2/'\n",
    "\n",
    "response = requests.get(\n",
    "    catalogue_base_url + 'works',\n",
    "    params={\n",
    "        'include': 'identifiers,subjects,production',\n",
    "        'pageSize': 100,\n",
    "        'query': 'typhoid',\n",
    "    },\n",
    ")\n",
    "if response.status_code != 200:\n",
    "  print('error', file = sys.stderr)\n",
    "response_data = response.json()\n",
    "for k, v in response_data.items():\n",
    "    if k == 'results': continue #there will be loads of this\n",
    "    print(f'{k}: {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time I updated this cell I got 1099 `totalResults`. Your results may differ, depending upon how Wellcome's collection has changed in the meantime. Anyway, this feels like a nice number of texts to start working with. Let's learn some more about them. We'll start by downloading the catalogue data for all of the pages of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a progress bar\n",
    "catalogue_bar = tqdm(\n",
    "  unit = 'pages',\n",
    "  total = response_data['totalPages'],\n",
    "  desc = 'downloading catalogue data',\n",
    ")\n",
    "\n",
    "#We already got the first page of results in the previous cell\n",
    "catalogue_bar.update(1)\n",
    "works = response_data['results']\n",
    "\n",
    "#Now we'll add all of the other pages of results to the list \"works\"\n",
    "while 'nextPage' in response_data:\n",
    "  response = requests.get(response_data['nextPage'])\n",
    "  catalogue_bar.update(1)\n",
    "  if response.status_code != 200:\n",
    "    print('error', file = sys.stderr)\n",
    "  response_data = response.json()\n",
    "  works.extend(response_data['results'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the catalogue data for our \"typhoid\" works, let's get a sense of what this covers. We'll just look at the contents of the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_display(works[0], expanded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a lot of data! We are interested in text about typhoid, so let's focus on the type of work that this is (is it something written, or something else, like a drawing or a photograph?) and the subject matter. We can use JSONPath to look this up.\n",
    "\n",
    "We'll start with the \"type\" of the work. The last entry in the above JSON is workType. The label and type look relevant. Let's examine the values that these can take across the whole collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... might want to add a cell about filtering out non-Wellcome items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell uses dumpCount from helper.py.\n",
    "#dumpCount takes a JSONPath query and a list of JSONL objects\n",
    "#It prints text describing query results\n",
    "\n",
    "print('workType types:')\n",
    "helper.dumpCount('$.workType.type', works)\n",
    "print()\n",
    "print('workType labels:')\n",
    "helper.dumpCount('$.workType.label', works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a range of types of works in our results. At the time of writing, 3/4 of the works are books and several others are of types that could reasonably have text (e.g. \"Archives and manuscripts\", \"Student dissertations\", \"E-books\", \"Manuscripts\", \"Journals\".\n",
    "\n",
    "Given that text is provided by an OCR pipeline, it is only printed texts that are likely to have online text available. So we filter down (for now) to just books, e-books and journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is quite Pythonic, but essentially is filtering works down to a list of just\n",
    "#books, e-books and journals. We cannot use JSONPath to do this because JSONPath\n",
    "#can only check values for the purpose of filtering lists, and works appears to\n",
    "#JSONPath code as single JSON objects (re e.g. https://stackoverflow.com/a/43737750)\n",
    "\n",
    "printed_works = list(filter(lambda x: x['workType']['label'] == 'Books' or\n",
    "                                      x['workType']['label'] == 'E-books' or\n",
    "                                      x['workType']['label'] == 'Journals', works))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the catalogue subject of a work is more complicated. Works in Wellcome Collection are classified according to a range of schemes. If we look in the above JSON, we can also see that the structure is fairly complex, involving a mixture of \"Subjects\" and \"Concepts\". Rather than unpick all this, we'll just look at a part of the structure to get a sense of how things are classified. We'll stick with the printed works here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', printed_works, 0.02)\n",
    "\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', printed_works, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have two columns of numbers.\n",
    "\n",
    "Before, we were dealing with formats. Each catalogue entry refers to a single physical object --- a book, a journal, a picture, etc etc --- and so it has only one format.\n",
    "\n",
    "Now, we are dealing with subjects. Each catalogue entry may have more than one subject.\n",
    "\n",
    "The \"entries\" column is counting catalogue entries. Because an entry can have multiple subjects, the sum of times that we see each subject, across all subjects in the corpus, is going to be higher than the number of entries in the corpus.\n",
    "\n",
    "Let's unpack this with a small example. To begin, we can look at the subjects in a sample of ten works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My original list was just the first 10 works in printed_works, but rather than rely on that staying the\n",
    "#same into the future, I now get the same works by identifier. I've also manipulated the list a bit to \n",
    "#make a better example.\n",
    "sample = helper.works_by_ids(printed_works,\n",
    "                               ['bxa3fqrw','f56ccxnd','jf55amap','pw7sr9zn','q5pqqysq','qzy6ufxp','rxyt9ncw','vqhzjwd5','ab2ncfmj', 'sqwwchy7'])\n",
    "\n",
    "display(Markdown('\\n'.join(helper.dump_labels(sample, '$.subjects[?(@.type==\"Subject\")]', 'subjects', '^Typhoid [Ff]ever$'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are two different ways of identifying the general subject of \"typhoid fever\" -- one spelling fever with a capital F and one with a lower case f. These two spellings also have distinct IDs. If we wanted to find all books with this general subject then we would have to use both spellings. Even then, we would have to watch out for cases like that copy of \"On typhoid fever\", which has both spellings.\n",
    "\n",
    "You may also notice that there are two copies of William Thomson's \"On typhoid fever\". As it happens, one of these copies has two different \"Typhoid fever\" subjects, and one has only one of them.\n",
    "\n",
    "But to return to our point about how to count things, and proportions of things. What we see here is a total of 10 printed works. These have varying numbers of subjects, totalling 18 (3 * 1 + 6 * 2 + 1 * 3 = 18).\n",
    "\n",
    "Let's now run our dumpCount function over the same ten works, first to get the titles, then to get the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.title', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it on the titles show us that \"On typhoid fever\" appears twice and all of the others appear once. \"On typhoid fever\" is therefore 20% of the sample.\n",
    "\n",
    "The numbers here are out of 10, because there are 10 works, and we only get one list because the number of titles equals the number of works.\n",
    "\n",
    "Now let's look at subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different numbers of works (10) and subjects (18), as we saw above. Because of this we get two lists: the \"entries\" list counts works and the \"hits\" list counts subjects.\n",
    "\n",
    "The left-hand \"entries\" column is still counting \"by work\" --- each number is out of 10, the number of works.\n",
    "\n",
    "4/10 works, or 40% of all works, have the subject \"Typhoid Fever\", and another 3/10, or 30% of all works, have the subject \"Typhoid fever\". \n",
    "\n",
    "The first work, \"Typhoid fever and chronic typhoid carriers\", has the subjects \"Typhoid Fever - epidemiology\" and \"Typhoid Fever - transmission\", so it effectively appears twice in the left-hand columns, once for each subject. All works will be counted once in this column for each subject that they have. Because of this, the total of entries in the left-hand column is greater than 10 --- in fact, it will be 18, the total number of subjects. If we count up the percentages in this column, they will come to 200%.\n",
    "\n",
    "The right-hand \"hits\" column is counting \"by subject\" --- each number is out of 18, the total number of subjects possessed by all of the books. Just as \"On typhoid fever\" appeared twice in our lists of titles, some subjects appear more than once when we list all of the subjects of all of the books.\n",
    "\n",
    "Note also that the inconsistent nature of the data leads to some misrepresentation. If we normalize by case, the proportions will change a little --- let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the subjects in a rough and ready way -- this normalizes label case but might not be consistent in other attributes, such as id\n",
    "#this is good enough for present purposes\n",
    "\n",
    "normalized_works = [deepcopy(work) for work in sample] #misnomer: we are only normalizing subject label\n",
    "for n_w in normalized_works:\n",
    "  subjects = n_w['subjects']\n",
    "  seen = set()\n",
    "  normalized_subjects = []\n",
    "  for subject in subjects:\n",
    "    if subject['type'] != 'Subject': continue #if it is not actually a subject, move on to the next subject\n",
    "    lowered = subject['label'].lower()\n",
    "    if lowered in seen: continue #if this work already has a subject with this label, move on to the next subject\n",
    "    subject['label'] = lowered\n",
    "    seen.add(lowered)\n",
    "    normalized_subjects.append(subject)\n",
    "  n_w['subjects'] = normalized_subjects\n",
    "display(Markdown('\\n'.join(helper.dump_labels(normalized_works, '$.subjects[?(@.type==\"Subject\")]', 'subjects', '^Typhoid [Ff]ever$'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general \"Typhoid Fever\" subject is now consistently \"typhoid fever\".\n",
    "\n",
    "We still have our same ten titles but now only 16 subjects because \"Typhoid fever: a history\" and the first copy of \"On typhoid fever\" no longer have the same subject label listed twice with different cases.\n",
    "\n",
    "So let's perform the same analysis with this slightly cleaner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', normalized_works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see easily see that 60% of the works have the most general \"typhoid fever\" subject, which is also 38% of all of the subjects covered.\n",
    "\n",
    "Which might be roughly what we would expect in a corpus based on a search for \"typhus\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This text applied to running the above cell on all works. If I stick with the printed_works version then it will need updating.]\n",
    "\n",
    "Straight away, we can see that both Subjects and Concepts are not available for about 1/5 of the collection (e.g. 204/1100 ( 19%) have no value).\n",
    "\n",
    "We can also see that there are a lot of possible values here -- so many that I've written the code to hide all results applying to less than 2% of the works on typhoid.\n",
    "\n",
    "We can also see that the phrase \"typhoid fever\" (with varying capitalization) covers 50% of the Subjects and 63% of the Concepts. This suggests that these specific values will get pretty good results in a search. What we cannot tell from this is how many of the works covered by other concepts are actually relevant.\n",
    "\n",
    "[This could be a good place to introduce the difference between Wellcome and non-Wellcome works and to see what effect filtering down to just Wellcome has.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty may be that Wellcome's catalogue includes texts not held by Wellcome. These have the potential to be classified differently.\n",
    "\n",
    "So let's assume that we are interested in searching works actually held by Wellcome itself and limit down to them.\n",
    "\n",
    "The way that was suggested to me to do this was to look for works held on either open shelves or in closed stores. This seems to make sense, although perhaps it needs a tweak for purely digital works such as E-books.\n",
    "\n",
    "For purposes of this notebook we won't worry about purely digital works, so lets filter down. For this purpose, we'll work with all format types again (not just printed works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All availability ids:')\n",
    "helper.dumpCount('$.availabilities[*].id', works)\n",
    "print()\n",
    "\n",
    "open_searcher   = json_parse(\"$.availabilities[?(@.id=='open-shelves')].id\")\n",
    "closed_searcher = json_parse(\"$.availabilities[?(@.id=='closed-stores')].id\")\n",
    "\n",
    "wellcome_works = list(filter(lambda x: len(open_searcher.find(x)) > 0 or len(closed_searcher.find(x)) > 0, works))\n",
    "print(f'{len(wellcome_works)}/{len(works)} works are available in closed and/or open stores (therefore held by Wellcome itself)')\n",
    "\n",
    "wellcome_printed = list(filter(lambda x: x['workType']['label'] == 'Books' or\n",
    "                                         x['workType']['label'] == 'E-books' or\n",
    "                                         x['workType']['label'] == 'Journals', wellcome_works))\n",
    "print(f'{len(wellcome_printed)} of these are *printed* works that may have OCR text available. These break down as:')\n",
    "helper.dumpCount('$.workType.label', wellcome_printed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done this, we can look again at concepts and subjects, to see what the coverage is like for the particular works that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subjects')\n",
    "#label of every member of the subjects array which has a type of Subject\n",
    "helper.dumpCount('$.subjects[?(@.type==\"Subject\")].label', wellcome_printed, 0.02)\n",
    "\n",
    "print()\n",
    "print('Concepts')\n",
    "#label of every node at any depth beneath subjects which has a type of concept\n",
    "helper.dumpCount('$.subjects..*[?(@.type==\"Concept\")].label', wellcome_printed, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that [analysis may change as I fiddle things around] the subjects \"Typhoid Fever - epidemiology\" and \"Typhoid fever\" cover 58% of our original search results as filtered down to printed texts held at Wellcome, or 76% for the concept \"Typhoid Fever\". Around 15% of works have neither subject nor concept, indicating that this is not explained only by a work not being held at Wellcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a sense of when and where these works were published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dates (by frequency)\")\n",
    "helper.dumpCount('$.production[*].dates[*].label', wellcome_printed)\n",
    "print()\n",
    "print(\"Dates (roughly ordered)\")\n",
    "empty, counter = helper.count('$.production[*].dates[*].label', wellcome_printed)\n",
    "print(sorted(counter.elements()))\n",
    "print()\n",
    "print(\"Places\")\n",
    "helper.dumpCount('$.production[*].places[*].label', wellcome_printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rough chart of cumulative dates')\n",
    "\n",
    "#Just grab the dates that are easy to pick up\n",
    "#This code will skip over an opening square bracket if there is one, then grab a 4 digit number if there is one\n",
    "#Otherwise it will reject the date as unusable\n",
    "#This means for example that '18' would be rejected\n",
    "#While '1900-1920' would be turned into just '1900'\n",
    "mismatch = 0\n",
    "searcher = json_parse('$.production[*].dates[*].label')\n",
    "matcher = re.compile(r'\\s*\\[?(\\d{4})(?:\\D|$)')\n",
    "filtered_results = Counter()\n",
    "for work in wellcome_printed:\n",
    "  results = searcher.find(work)\n",
    "\n",
    "  #a work may have more than one date, we just take the earliest one\n",
    "  first_date = 99999 #a 4 digit number must be lower than this\n",
    "  for result in results:\n",
    "    match = matcher.match(result.value)\n",
    "    if match:\n",
    "      x = int(match.group(1))\n",
    "      if x < first_date:\n",
    "        first_date = x\n",
    "  if first_date == 99999: #no date found\n",
    "    mismatch += 1\n",
    "  else:\n",
    "    filtered_results[first_date] += 1\n",
    "\n",
    "filtered_results = sorted(filtered_results.items())\n",
    "first_year = filtered_results[0][0]\n",
    "final_year = filtered_results[-1][0]\n",
    "total = 0\n",
    "cumulative = {}\n",
    "for year, count in filtered_results:\n",
    "  total += count\n",
    "  cumulative[year] = total\n",
    "rounded_up_total = math.ceil(total / 100.0) * 100\n",
    "\n",
    "ax = seaborn.lineplot(cumulative)\n",
    "ax.set(\n",
    "  xlabel = 'Year',\n",
    "  ylabel = 'Total works',\n",
    "  ylim = (0, rounded_up_total),\n",
    ")\n",
    "plt.show()\n",
    "print(f'{mismatch} works have no usable date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have explored what the catalogue can tell us a little, and got a rough sense of the range of texts that we might be able to work with. The next step is to find out which ones actually have digitised text available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
